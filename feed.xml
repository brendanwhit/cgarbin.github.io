<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="https://cgarbin.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://cgarbin.github.io/" rel="alternate" type="text/html" /><updated>2022-12-20T08:11:47-05:00</updated><id>https://cgarbin.github.io/feed.xml</id><title type="html">Christian Garbin’s personal blog</title><subtitle>Articles collected during my machine learning master&apos;s and Ph.D. work at Florida Atlantic University.
</subtitle><author><name>Christian Garbin</name></author><entry><title type="html">Writing good Jupyter notebooks</title><link href="https://cgarbin.github.io/writing-good-jupyter-notebooks/" rel="alternate" type="text/html" title="Writing good Jupyter notebooks" /><published>2022-09-19T00:00:00-04:00</published><updated>2022-09-19T00:00:00-04:00</updated><id>https://cgarbin.github.io/writing-good-jupyter-notebooks</id><content type="html" xml:base="https://cgarbin.github.io/writing-good-jupyter-notebooks/">&lt;p&gt;&lt;a href=&quot;https://jupyter.org/&quot;&gt;Jupyter notebooks&lt;/a&gt; are an excellent tool for data scientists and machine learning practitioners. However, if not approached with a few techniques, they can turn into a pile of unintelligible, unmaintainable code.&lt;/p&gt;

&lt;p&gt;This post will discuss some techniques I use to write good Jupyter notebooks. We will start with a notebook that is not wrong but is not well written. We will progressively change it until we arrive at a good notebook.&lt;/p&gt;

&lt;p&gt;But first, what is a good Jupyter notebook? Good notebooks have the following properties:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;They are organized logically, with sections clearly delineated and named.&lt;/li&gt;
  &lt;li&gt;They have important assumptions and decisions spelled out.&lt;/li&gt;
  &lt;li&gt;Their code is easy to understand.&lt;/li&gt;
  &lt;li&gt;Their code is flexible (easy to modify).&lt;/li&gt;
  &lt;li&gt;Their code is resilient (hard to break).&lt;/li&gt;
&lt;/ul&gt;

&lt;!--more--&gt;

&lt;p class=&quot;notice--info&quot;&gt;This post is adapted from a guest lecture I gave to &lt;a href=&quot;https://www.ogemarques.com/&quot;&gt;Dr. Marques’&lt;/a&gt; data science class. If you are pressed for time, check out the &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/writing-good-jupyter-notebooks&quot;&gt;GitHub repository&lt;/a&gt;, starting with the &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/writing-good-jupyter-notebooks/blob/master/presentation.pdf&quot;&gt;presentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We will use as an example a notebook that attempts to answer the question &lt;em&gt;“is there gender discrimination in the salaries of an organization?”&lt;/em&gt; Our dataset is a list of salaries and other attributes from that organization. We will start from the first step in any data project, exploratory data analysis (EDA), clean up the dataset, and finally, attempt to answer the question.&lt;/p&gt;

&lt;p&gt;To illustrate how to go from a notebook that is not wrong but is also not good, we will go through the following steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#step-1---the-original-notebook&quot;&gt;Step 1&lt;/a&gt;: the original notebook, the one that lacks structure and proper coding practices.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#step-2---add-a-description-organize-into-sections-add-exploratory-data-analysis&quot;&gt;Step 2&lt;/a&gt;: adds a description, organize into sections, add exploratory data analysis.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#step-3---make-data-cleanup-more-explicit-and-explain-why-specific-numbers-were-chosen&quot;&gt;Step 3&lt;/a&gt;: make data cleanup more explicit and explain why specific numbers were chosen (the assumptions behind them).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#step-4---make-the-code-more-flexible-and-more-difficult-to-break&quot;&gt;Step 4&lt;/a&gt;: make the code more flexible with constants and make the code more difficult to break.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#step-5---make-the-graphs-easier-to-read&quot;&gt;Step 5&lt;/a&gt;: make the graphs easier to read.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#step-6---describe-the-limitations-of-the-conclusion&quot;&gt;Step 6&lt;/a&gt;: describe the limitations of the conclusion.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;step-1---the-original-notebook&quot;&gt;Step 1 - The original notebook&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/writing-good-jupyter-notebooks/blob/master/salary-discrimination-by-gender-step-1.ipynb&quot;&gt;This is the original notebook&lt;/a&gt;. It is technically correct, but far from what is acceptable for a project of this importance.&lt;/p&gt;

&lt;p&gt;The first hint of a problem is the structure of the notebook: it doesn’t have any. It’s a collection of cells, one after the other.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-09-19/step-1.drawio.png&quot; alt=&quot;Step 1 - This notebook has no structure and other problems&quot; class=&quot;align-center&quot; style=&quot;width:66%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Other problems with this notebook:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;There is no description of what the notebook is about.&lt;/li&gt;
  &lt;li&gt;There is no exploratory data analysis (EDA) to explain why we can (or cannot) trust our data.&lt;/li&gt;
  &lt;li&gt;The data cleanup (the magic numbers) is not explained, e.g. why were those numbers used?&lt;/li&gt;
  &lt;li&gt;Because of the magic numbers, the code is not flexible. We don’t know where they are used and the effect of changing one of them.&lt;/li&gt;
  &lt;li&gt;There is no explanation for the code blocks.&lt;/li&gt;
  &lt;li&gt;There is no explanation for the conclusion.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will fix some of the issues in the next step.&lt;/p&gt;

&lt;h2 id=&quot;step-2---add-a-description-organize-it-into-sections-and-add-exploratory-data-analysis&quot;&gt;Step 2 - Add a description, organize it into sections, and add exploratory data analysis&lt;/h2&gt;

&lt;p&gt;Starting in this step, we will make incremental changes to the notebook. Each change will bring us closer to a good notebook. Changes from the previous step are highlighted with a “REWORK NOTE” comment and an explanation of what has changed. Here is an example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-09-19/rework-note.drawio.png&quot; alt=&quot;Rework note&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this step, we make the following improvements:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Add a clear “what is this notebook about?” description.&lt;/li&gt;
  &lt;li&gt;Add an exploratory data analysis (EDA) section.&lt;/li&gt;
  &lt;li&gt;Split the notebook into sections.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-09-19/step-2.drawio.png&quot; alt=&quot;Step 2 - Add sections headers and EDA&quot; class=&quot;align-center&quot; style=&quot;width:66%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/writing-good-jupyter-notebooks/blob/master/salary-discrimination-by-gender-step-2.ipynb&quot;&gt;This is the reworked notebook&lt;/a&gt;. It is better, but we can still improve it:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Make the data cleanup more explicit.&lt;/li&gt;
  &lt;li&gt;Explain what the code blocks are doing.&lt;/li&gt;
  &lt;li&gt;Explain why specific numbers were chosen (the assumptions behind them).&lt;/li&gt;
  &lt;li&gt;Make the graphs easier to read.&lt;/li&gt;
  &lt;li&gt;Make the code more flexible with constants.&lt;/li&gt;
  &lt;li&gt;Make the code more resilient (harder to break).&lt;/li&gt;
  &lt;li&gt;Describe the limitations of the conclusion.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will fix some of the issues in the next step.&lt;/p&gt;

&lt;h2 id=&quot;step-3---make-data-cleanup-more-explicit-and-explain-why-specific-numbers-were-chosen&quot;&gt;Step 3 - Make data cleanup more explicit and explain why specific numbers were chosen&lt;/h2&gt;

&lt;p&gt;In this step, we make the following improvements:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Make the data cleanup more explicit.&lt;/li&gt;
  &lt;li&gt;Explain why specific numbers were chosen (the assumptions behind them).&lt;/li&gt;
  &lt;li&gt;Explain what the code blocks are doing.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the following figure, we explain why we are removing all employees that are 66 or older and add a reference to back up our decision (the hyperlink in the text). We also explain what this piece of code is doing in detail.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-09-19/step-3.drawio.png&quot; alt=&quot;Step 3 - Making data cleanup more explicit&quot; class=&quot;align-center&quot; style=&quot;width:66%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/writing-good-jupyter-notebooks/blob/master/salary-discrimination-by-gender-step-3.ipynb&quot;&gt;This is the reworked notebook&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;step-4---make-the-code-more-flexible-and-more-difficult-to-break&quot;&gt;Step 4 - Make the code more flexible and more difficult to break&lt;/h2&gt;

&lt;p&gt;In this step, we make the following improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Make the code more flexible with constants. If we need to change decisions, for example, the age cutoff, we have only one place to change.&lt;/li&gt;
  &lt;li&gt;Make the code more difficult to break. By following patterns, we reduce the chances of introducing bugs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this piece of code, we remove everyone who made less than the minimum age working full time (see the &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/writing-good-jupyter-notebooks/blob/master/salary-discrimination-by-gender-step-4.ipynb&quot;&gt;notebook&lt;/a&gt; for details).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-09-19/step-4.1.drawio.png&quot; alt=&quot;Step 4 - Salary cutoff&quot; class=&quot;align-center&quot; style=&quot;width:66%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are a few notable items in this code:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We use a constant if we need to make changes later (more flexible code).&lt;/li&gt;
  &lt;li&gt;We use a generic name for the constant (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SALARY_CUTOFF&lt;/code&gt;), so we don’t need to change it later if we change the cutoff value. If we had named it something more specific, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MINIMUM_WAGE&lt;/code&gt;, we would need to change the constant name if we changed the value. This makes the code less flexible and less resilient.&lt;/li&gt;
  &lt;li&gt;We don’t modify the original data. We create a filter instead, so we can see the effect of each filter separately and backtrack one change at a time if we need to.&lt;/li&gt;
  &lt;li&gt;The filter variable also has a generic name (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;low_salaries&lt;/code&gt;), for the same reasons we used a generic name for the constant.&lt;/li&gt;
  &lt;li&gt;We print the results of the operation (the cutoff value and how many items it removed from the dataset), so we can discuss with the domain experts if our decision makes sense. For example, we could ask an HR representative if they expected to see this many employers removed when we set this salary cutoff. It may catch errors in the dataset or in the code.&lt;/li&gt;
&lt;/ol&gt;

&lt;p class=&quot;notice&quot;&gt;Regarding the last item, printing the operation results: &lt;strong&gt;we, the data scientists, may not be the domain experts&lt;/strong&gt;. In this example, the domain experts are the HR and legal departments. We need to engage them to validate our decisions. Simple things, like printing the effect of filtering the data (how many employees were removed) help validate the decisions with the domain experts.&lt;/p&gt;

&lt;p&gt;When we clean up the age column, we keep using the same patterns:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We create a filter for the data we want to exclude, as we did for the salary filter.&lt;/li&gt;
  &lt;li&gt;We follow a pattern for the variable name. The salary one was named &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SALARY_CUTOFF&lt;/code&gt;, so this one is also suffixed with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;..._CUTOFF&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;We choose a generic variable name. If we name it something more specific, e.g. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RETIRED_AGE&lt;/code&gt; and decide to change the age cutoff later, the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RETIRED_&lt;/code&gt; part may no longer make sense. A generic name (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AGE_CUTOFF&lt;/code&gt;) requires only a change to the value, making the code more resilient.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-09-19/step-4.2.drawio.png&quot; alt=&quot;Step 4 - Age cutoff, following the same patterns as the salary cutoff&quot; class=&quot;align-center&quot; style=&quot;width:66%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;With all the filters in place, we can clean up the data in one step. Because all the filters we created are to exclude data, we can confidently negate all of them to get the data we want to keep. If we use different types of filters (exclude and include), we have to carefully think about how to apply each of them, opening the door for bugs.&lt;/p&gt;

&lt;p class=&quot;notice&quot;&gt;This is an important concept: don’t make your brain hold more information than it absolutely has to (don’t create &lt;a href=&quot;https://en.wikipedia.org/wiki/Cognitive_load#Extraneous&quot;&gt;extraneous cognitive load&lt;/a&gt;). If we follow a pattern, we have only one thing to remember, the pattern itself.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-09-19/step-4.3.drawio.png&quot; alt=&quot;Step 4 - Applying the filters to clean up the data&quot; class=&quot;align-center&quot; style=&quot;width:80%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/writing-good-jupyter-notebooks/blob/master/salary-discrimination-by-gender-step-4.ipynb&quot;&gt;This is the reworked notebook&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;step-5---make-the-graphs-easier-to-read&quot;&gt;Step 5 - Make the graphs easier to read&lt;/h2&gt;

&lt;p&gt;In this step, we make the graphs easier to read.&lt;/p&gt;

&lt;p&gt;First, we add transparency when plotting multiple variables on the same graph.&lt;/p&gt;

&lt;p&gt;This is the &lt;a href=&quot;https://seaborn.pydata.org/generated/seaborn.pairplot.html&quot;&gt;pairplot &lt;/a&gt; from the previous step, without transparency:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-09-19/step-5.1.drawio.png&quot; alt=&quot;Step 5 - Pairplot without transparency&quot; class=&quot;align-center&quot; style=&quot;width:66%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And this is the pairplot with transparency:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-09-19/step-5.2.drawio.png&quot; alt=&quot;Step 5 - Pairplot with transparency&quot; class=&quot;align-center&quot; style=&quot;width:66%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Adding transparency lets us see the clusters of data, the areas where we have many data points, as opposed to the places where we have few data points. It helps identifies patterns in the data.&lt;/p&gt;

&lt;p&gt;Another technique to make graphs readable is to &lt;a href=&quot;https://en.wikipedia.org/wiki/Data_binning&quot;&gt;bin the data&lt;/a&gt;. This is the graph from the previous step that plots age vs. salary:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-09-19/step-5.3.drawio.png&quot; alt=&quot;Step 5 - Salary by age before binning&quot; class=&quot;align-center&quot; style=&quot;width:66%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is impossible to see any pattern in such a graph. To make it more legible, we will &lt;a href=&quot;https://en.wikipedia.org/wiki/Data_binning&quot;&gt;bin the data&lt;/a&gt;. But the question is “what bins make sense for this case?” Since we are analyzing salaries, we chose 22 as our first bin because this is usually the &lt;a href=&quot;https://nonpartisaneducation.org/Review/Resources/Int__lHigherEd_AppendixA.pdf&quot;&gt;age of graduation&lt;/a&gt;. After that, we will bin every five years for the first years to account for rapid promotions and rises that happen at the start of a career, then bin every ten years for later stages in the career, where promotions are rarer. We also document those assumptions clearly to discuss them with the domain experts.&lt;/p&gt;

&lt;p&gt;This is the new graph:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-09-19/step-5.4.drawio.png&quot; alt=&quot;Step 5 - Salary by age after binning&quot; class=&quot;align-center&quot; style=&quot;width:66%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/writing-good-jupyter-notebooks/blob/master/salary-discrimination-by-gender-step-5.ipynb&quot;&gt;This is the reworked notebook&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;step-6---describe-the-limitations-of-the-conclusion&quot;&gt;Step 6 - Describe the limitations of the conclusion&lt;/h2&gt;

&lt;p&gt;We now have a good notebook. It is organized in sections, uses constants to make the code more understandable and resilient, the graphs are well formatted, and we added explanations for all assumptions and decisions.&lt;/p&gt;

&lt;p&gt;We are now at the last step, where we present the conclusion to the original question, &lt;em&gt;“is there gender discrimination in the salaries of an organization?”&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In real life, the data we have is not perfect and complex questions don’t always have simple answers. And that’s the case here. We have a few limitations that prevent us from giving a definitive answer to the question. But we have enough to spur some action. Our job at this point is to document what we found and the limitations of our analysis.&lt;/p&gt;

&lt;p&gt;In the conclusion section, we clearly document:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;That we used proxy variables.&lt;/li&gt;
  &lt;li&gt;Despite the dataset’s limitations, we have tentative conclusions.&lt;/li&gt;
  &lt;li&gt;That we need more precise data, but at the same time, we have enough to take action (and avoid &lt;a href=&quot;https://en.wikipedia.org/wiki/Analysis_paralysis]&quot;&gt;analysis paralysis&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2022-09-19/step-6.drawio.png&quot; alt=&quot;Step 6 - Conclusions and limitations&quot; class=&quot;align-center&quot; style=&quot;width:66%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/writing-good-jupyter-notebooks/blob/master/salary-discrimination-by-gender-step-6.ipynb&quot;&gt;This is the final notebook&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We write notebooks for our stakeholders, not for ourselves.&lt;/p&gt;

&lt;p&gt;To write good notebooks, we need to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Organize them logically so that the stakeholders can follow the analysis.&lt;/li&gt;
  &lt;li&gt;Make the code easy to understand, easy to change (flexible), and hard to break (resilient), so we can modify it confidently as we review the results with the stakeholders.&lt;/li&gt;
  &lt;li&gt;Spell out critical assumptions and decisions so stakeholders can validate them (or challenge them).&lt;/li&gt;
  &lt;li&gt;Clearly document the limitations of the analysis so stakeholders can decide if they are acceptable or not.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;running-the-examples&quot;&gt;Running the examples&lt;/h2&gt;

&lt;p&gt;The notebooks are available on &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/writing-good-jupyter-notebooks&quot;&gt;this GitHub repository&lt;/a&gt;.&lt;/p&gt;</content><author><name>Christian Garbin</name></author><category term="jupyter-notebooks" /><category term="python" /><category term="data-science" /><summary type="html">How to write well-structured, understandable, flexible, and resilient Jupyter notebooks.</summary></entry><entry><title type="html">Vision transformer properties</title><link href="https://cgarbin.github.io/vision-transformers-properties/" rel="alternate" type="text/html" title="Vision transformer properties" /><published>2022-07-23T00:00:00-04:00</published><updated>2022-07-23T00:00:00-04:00</updated><id>https://cgarbin.github.io/vision-transformers-properties</id><content type="html" xml:base="https://cgarbin.github.io/vision-transformers-properties/">&lt;p&gt;Transformers crossed over from natural language into computer vision in a few low-key steps until the &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;An Image is Worth 16x16 Words&lt;/a&gt; paper exploded into the machine learning scene late in 2020.&lt;/p&gt;

&lt;p&gt;Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) were the dominant architectures in computer vision tasks until transformers arrived on the scene. When I started studying vision transformers, I assumed they were a replacement for CNNs and RNNs. I learned that they are more than that. They are a fundamentally different approach to the problem, resulting in some interesting properties.&lt;/p&gt;

&lt;p&gt;In this article we will review transformers’ properties in computer vision tasks that set them apart from CNNs and RNNs.&lt;/p&gt;

&lt;p&gt;If you haven’t read the &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;paper&lt;/a&gt; yet, start with the accompanying blog post &lt;a href=&quot;https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html&quot;&gt;Google AI Blog: Transformers for Image Recognition at Scale&lt;/a&gt;. It has a nice animation and covers the topics in the paper at a higher level. This &lt;a href=&quot;https://www.youtube.com/watch?v=DVoHvmww2lQ&quot;&gt;six-minute video from AI Coffee Break with Letitia  &lt;i class=&quot;fab fa-youtube&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt; is an excellent introduction to the paper or a refresher if it has been a while since you read it.&lt;/p&gt;

&lt;p&gt;If this is your first encounter with transformers, start with transformers in natural language processing to learn the fundamental concepts, then come back to vision transformers. Check out &lt;a href=&quot;/understanding-transformers-in-one-morning/&quot;&gt;Understanding transformers in one morning&lt;/a&gt; if you are not yet familiar with the topic.&lt;/p&gt;

&lt;h2 id=&quot;how-transformers-process-images&quot;&gt;How transformers process images&lt;/h2&gt;

&lt;p&gt;First, a brief review of how transformers were adapted from natural language processing (NLP) to computer vision.&lt;/p&gt;

&lt;p&gt;Transformers operate on sequences. In NLP the sequence is a piece of text. For example, in the sentence “the cat refused to eat the food because it was cold” we can correlate the word “it” to “food” (not “cat”) and use that to illustrate the concept of “attention.” It is easy to conceive text as a sequence of words and imagine transformer concepts that way.&lt;/p&gt;

&lt;p&gt;But what is a “sequence” in computer vision? That is the first significant difference between transformers in computer vision and transformers in NLP.&lt;/p&gt;

&lt;p&gt;A naive solution would be to treat an image as a sequence of pixels. The problem with this approach is that it generates humongous sequences. A 256 x 256 RGB image, commonly used to train models, results in a sequence of 196,608 pixels (256 x 256 x 3 RGB channels). This large sequence would require too many computing resources for training and inference. To help visualize: a 400-page book has about 200,000 words. In this one-to-one mapping of pixels to words, it would be the equivalent of feeding that book to a transformer network at once.&lt;/p&gt;

&lt;p&gt;To make the problem tractable, &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;An Image is Worth 16x16 Words&lt;/a&gt; partitions images into squares called &lt;em&gt;patches&lt;/em&gt;. Each patch is the equivalent of a token in an NLP transformer. Back to the 256 x 256 image, partitioning it into 16 x 16 squares results in 256 patches (tokens). Each patch is still a large number of pixels, but the problem is more tractable now because the number of tokens is much smaller.&lt;/p&gt;

&lt;p&gt;In addition to the patches, the network has one more token, the class token. This token is the image classification (“cat”, “dog”, …). Beyond that, the transformer network in &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;An Image is Worth 16x16 Words&lt;/a&gt; is the same as the transformers used in natural language processing. In the words of the paper, “&lt;em&gt;The “Base” and “Large” models are directly adopted from &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot;&gt;BERT&lt;/a&gt;&lt;/em&gt;”.&lt;/p&gt;

&lt;p&gt;The picture below, from &lt;a href=&quot;https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html&quot;&gt;Google’s blog post&lt;/a&gt;, shows the network architecture. Token zero is the class token. The patches are extracted from the image and used as tokens. This transformer is known as &lt;strong&gt;&lt;em&gt;ViT&lt;/em&gt;&lt;/strong&gt;, the vision transformer. The term &lt;em&gt;ViT&lt;/em&gt; is commonly used in the literature to refer to this architecture.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/images/2022-07-23/vit-architecture.gif&quot; alt=&quot;ViT architecture&quot; /&gt;&lt;figcaption&gt;
      The vision transformer (ViT) architecture from &lt;a href=&quot;https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html&quot;&gt;Google’s blog post&lt;/a&gt;

    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;how-are-transformers-different-from-cnns-in-computer-vision&quot;&gt;How are transformers different from CNNs in computer vision?&lt;/h2&gt;

&lt;p&gt;Convolutional neural networks (CNN) work in small image areas. The learned weights are related to that small area, as shown in this picture from &lt;a href=&quot;https://arxiv.org/abs/1906.05909&quot;&gt;Stand-Alone Self-Attention in Vision Models&lt;/a&gt;.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/images/2022-07-23/cnn-locality.png&quot; alt=&quot;CNN locality inductive bias&quot; /&gt;&lt;figcaption&gt;
      CNN locality inductive bias &lt;a href=&quot;https://arxiv.org/abs/1906.05909&quot;&gt;Stand-Alone Self-Attention in Vision Models&lt;/a&gt;

    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In other words, the concept of “locality” (pixels closer to each other are related) is part of the CNN architecture as a &lt;em&gt;prior&lt;/em&gt;, or &lt;em&gt;&lt;a href=&quot;https://towardsdatascience.com/the-inductive-bias-of-ml-models-and-why-you-should-care-about-it-979fe02a1a56&quot;&gt;inductive bias&lt;/a&gt;&lt;/em&gt;, a piece of knowledge that the network creators embedded into the network’s architecture. This piece of knowledge makes assumptions about what the best solution is for a specific problem. Perhaps there are better ways to solve the problem, but we are constraining the solution space to the inductive biases that are part of the network architecture.&lt;/p&gt;

&lt;p&gt;On the other hand, a transformer network doesn’t have such inductive biases embedded into its architecture. For example, It has to learn that “locality” is a good thing in computer vision problems on its own.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;This lack of inductive bias in the network architecture is a fundamental difference between transformers and CNNs&lt;/strong&gt;. In more practical terms, a transformer network does not make assumptions about the structure of the problem. As a result of that, the network has to learn the concepts.&lt;/p&gt;

&lt;p&gt;Eventually, the transformer network does &lt;a href=&quot;http://jbcordonnier.com/posts/attention-cnn/&quot;&gt;learn convolutions&lt;/a&gt; and locality. The picture below (from &lt;a href=&quot;https://arxiv.org/pdf/2010.11929.pdf&quot;&gt;An Image is Worth 16x16 Words&lt;/a&gt;) shows the size of the image area attended by each head in each layer. In the lower layers (left), some heads attend to pixels close to each other (bottom of the graph), and other heads attend to pixels further away (top of the graph). As we move up in the layers (right of the graph), heads attend to pixels farther out in the image area (top of the graph). In other words, lower layers have both local and global attention, while higher layers have global attention. The network was not told to behave this way. It learned this attention pattern on its own.&lt;/p&gt;

&lt;p&gt;In the words of the authors:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This “attention distance” is analogous to receptive field size in CNNs. We find that some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model. Other attention heads have consistently small attention distances in the low layers. This highly localized attention is less pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right), suggesting that it may serve a similar function as early convolutional layers in CNNs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;small&quot;&gt;&lt;cite&gt;&lt;a href=&quot;https://arxiv.org/pdf/2010.11929.pdf&quot;&gt;An Image is Worth 16x16 Words&lt;/a&gt;&lt;/cite&gt;&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/images/2022-07-23/vit-head-attention.png&quot; alt=&quot;ViT head attention by layer&quot; /&gt;&lt;figcaption&gt;
      ViT head attention by layer – &lt;a href=&quot;https://arxiv.org/pdf/2010.11929.pdf&quot;&gt;An Image is Worth 16x16 Words&lt;/a&gt;

    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;fewer-assumptions--more-interesting-solutions&quot;&gt;Fewer assumptions → more interesting solutions&lt;/h2&gt;

&lt;p&gt;If, in the end, transformers learn convolutions and locality anyway, what have we gained by using transformers for computer vision? Why go through all the trouble of training transformers to do what CNNs do from the start?&lt;/p&gt;

&lt;p&gt;In the words of Lucas Beyer (&lt;a href=&quot;https://youtu.be/BP5CM0YxbP8?t=3295&quot;&gt;Standford CS 25 lecture&lt;/a&gt;), one of the technical contributors to &lt;a href=&quot;https://arxiv.org/pdf/2010.11929.pdf&quot;&gt;ViT&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[W]e want the model to have as little of our thinking built-in, because what we may think that is good to solve the task may actually not be the best to solve the task. … [W]e want to encode as little as possible into the model, such that if we just throw massive amounts of data in a difficult task at it, it might think things that are even better than [what we would have assumed]… Ideally, we want [a] model that is powerful enough to learn about this concept [locality] itself, if it’s useful to solve the task. If it’s not useful to solve the task, then if we had put it in, there is no way for the model not to do this.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;small&quot;&gt;&lt;cite&gt;Lucas Beyer&lt;/cite&gt; – &lt;a href=&quot;https://youtu.be/BP5CM0YxbP8?t=3295&quot;&gt;Standford CS 25 lecture&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-else-do-transformers-learn-on-their-own&quot;&gt;What else do transformers learn on their own?&lt;/h2&gt;

&lt;p&gt;So, transformers learned to behave like CNNs. What else could they be learning on their own? By changing how a transformer model is trained, &lt;a href=&quot;https://arxiv.org/pdf/2104.14294.pdf&quot;&gt;Emerging Properties in Self-Supervised Vision Transformers&lt;/a&gt; found out that:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[W]e make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;small&quot;&gt;&lt;cite&gt;&lt;a href=&quot;https://arxiv.org/pdf/2104.14294.pdf&quot;&gt;Emerging Properties in Self-Supervised Vision Transformers&lt;/a&gt;&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;More concretely, when trained to perform object classification, transformers also learn object segmentation on their own, as shown in the following picture from the paper (for a more lively demonstration, see their &lt;a href=&quot;https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/&quot;&gt;blog post&lt;/a&gt;).&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/images/2022-07-23/transformer-segmentation.png&quot; alt=&quot;Transformer segmentation&quot; /&gt;&lt;figcaption&gt;
      Transformer segmentation – &lt;a href=&quot;https://arxiv.org/pdf/2104.14294.pdf&quot;&gt;Emerging Properties in Self-Supervised Vision Transformers&lt;/a&gt;

    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Segmenting an image requires some understanding of what the objects are, i.e., understanding the semantics of an image and not just treating it as a collection of pixels. The fact that the transformer model is segmenting the image indicates that it is also extracting semantic meanings. From their &lt;a href=&quot;https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/&quot;&gt;blog post&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;DINO learns a great deal about the visual world. By discovering object parts and shared characteristics across images, the model learns a feature space that exhibits a very interesting structure. If we embed ImageNet classes using the features computed using DINO, we see that they organize in an interpretable way, with similar categories landing near one another. This suggests that the model managed to connect categories based on visual properties, a bit like humans do.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;small&quot;&gt;&lt;cite&gt;&lt;a href=&quot;https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/&quot;&gt;Advancing the state of the art in computer vision with self-supervised Transformers and 10x more efficient training&lt;/a&gt;&lt;/cite&gt;&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/images/2022-07-23/transformer-class-separation.gif&quot; alt=&quot;Transformer class separation&quot; /&gt;&lt;figcaption&gt;
      Transformer class separation – &lt;a href=&quot;https://ai.facebook.com/blog/dino-paws-computer-vision-with-self-supervised-transformers-and-10x-more-efficient-training/&quot;&gt;Advancing the state of the art in computer vision with self-supervised Transformers and 10x more efficient training&lt;/a&gt;

    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2105.10497&quot;&gt;Intriguing Properties of Vision Transformers&lt;/a&gt; explores more properties of vision transformers, such as dealing with occlusion better than CNNs.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/images/2022-07-23/intriguing-properties-occlusion.png&quot; alt=&quot;Transformer deal with occlusion better than CNNs&quot; /&gt;&lt;figcaption&gt;
      Transformer deal with occlusion better than CNNs – &lt;a href=&quot;https://arxiv.org/abs/2105.10497&quot;&gt;Intriguing Properties of Vision Transformers&lt;/a&gt;

    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;They introduce a “shape token” to train transformers to be more shape-biased than they naturally are to get automated object segmentation (rightmost column in the picture below).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The [results] show that properly trained ViT models offer shape-bias nearly as high as the human’s ability to recognize shapes. This leads us to question if positional encoding is the key that helps ViTs achieve high performance under severe occlusions (as it can potentially allow later layers to recover the missing information with just a few image patches given their spatial ordering).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;small&quot;&gt;&lt;cite&gt;&lt;a href=&quot;https://arxiv.org/abs/2105.10497&quot;&gt;Intriguing Properties of Vision Transformers&lt;/a&gt;&lt;/cite&gt;&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/images/2022-07-23/intriguing-properties-segmentation.png&quot; alt=&quot;Transformer segmentation with shape token&quot; /&gt;&lt;figcaption&gt;
      Transformer segmentation with shape token better than CNNs – &lt;a href=&quot;https://arxiv.org/abs/2105.10497&quot;&gt;Intriguing Properties of Vision Transformers&lt;/a&gt;

    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2105.07581&quot;&gt;Vision Transformers are Robust Learners&lt;/a&gt; doesn’t have fancy pictures to illustrate what they found, but the results are no less interesting. They found out that without any specific training, vision transformers can cope with image perturbations better than CNNs.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[W]e study the robustness of the Vision Transformer … against common corruptions and perturbations, distribution shifts, and natural adversarial examples. … [W]ith fewer parameters and similar dataset and pre-training combinations, ViT gives a top-1 accuracy of 28.10% on &lt;a href=&quot;https://arxiv.org/pdf/1907.07174.pdf&quot;&gt;ImageNet-A&lt;/a&gt; which is 4.3x higher than a comparable variant of &lt;a href=&quot;https://arxiv.org/abs/1912.11370&quot;&gt;BiT&lt;/a&gt;. Our analyses on image masking, Fourier spectrum sensitivity, and spread on discrete cosine energy spectrum reveal intriguing properties of ViT attributing to improved robustness.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;small&quot;&gt;&lt;cite&gt;&lt;a href=&quot;https://arxiv.org/abs/2105.07581&quot;&gt;Vision Transformers are Robust Learners&lt;/a&gt;&lt;/cite&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-do-vision-transformers-perform-better-than-cnns&quot;&gt;Why do vision transformers perform better than CNNs?&lt;/h2&gt;

&lt;p&gt;We are still in the early stages of understanding the differences between CNNs and vision transformers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.08810&quot;&gt;Do Vision Transformers See Like Convolutional Neural Networks?&lt;/a&gt; found significant differences in the structure of vision transformers and CNNs.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[We use] &lt;a href=&quot;https://arxiv.org/abs/1905.00414&quot;&gt;CKA&lt;/a&gt; to study the internal representation structure of each model. … Figure 1 [below] shows the results as a heatmap, for multiple ViTs and ResNets. We observe clear differences between the internal representation structure between the two model architectures: (1) ViTs show a much more uniform similarity structure, with a clear grid like structure (2) lower and higher layers in ViT show much greater similarity than in the ResNet, where similarity is divided into different (lower/higher) stages.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;small&quot;&gt;&lt;cite&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.08810&quot;&gt;Do Vision Transformers See Like Convolutional Neural Networks?&lt;/a&gt;&lt;/cite&gt;&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/images/2022-07-23/do-transformers-see-like-cnns-figure-1.png&quot; alt=&quot;Transformer vs. ResNet internal representation&quot; /&gt;&lt;figcaption&gt;
      Transformer vs. ResNet internal representation – &lt;a href=&quot;https://arxiv.org/abs/2108.08810&quot;&gt;Do Vision Transformers See Like Convolutional Neural Networks?&lt;/a&gt;

    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;This is the first significant difference between vision transformers and CNNs.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[T]hese results suggest that (i) ViT lower layers compute representations in a different
way to lower layers in the ResNet, (ii) ViT also more strongly propagates representations between lower and higher layers (iii) the highest layers of ViT have quite different representations to ResNet.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;small&quot;&gt;&lt;cite&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.08810&quot;&gt;Do Vision Transformers See Like Convolutional Neural Networks?&lt;/a&gt;&lt;/cite&gt;&lt;/p&gt;

&lt;p&gt;A possible explanation for this structural difference is how the transformer layers learn to aggregate spatial information. CNNs have fixed receptive fields (encoded in the kernel sizes and sequences of layers). Transformers do not have this prior knowledge of “receptive fields” for an image. They have to learn that spatial relations are important in image processing.&lt;/p&gt;

&lt;p&gt;The experiments in the paper confirmed the observation in &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot;&gt;the original ViT paper&lt;/a&gt; that transformers eventually settle in a structure where lower layers learn to pay attention locally and globally, while higher layers learn to pay attention globally.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[E]ven in the lowest layers of ViT, self-attention layers have a mix of local heads (small distances) and global heads (large distances). This is in contrast to CNNs, which are hardcoded to attend only locally in the lower layers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;small&quot;&gt;&lt;cite&gt;&lt;a href=&quot;https://arxiv.org/abs/2108.08810&quot;&gt;Do Vision Transformers See Like Convolutional Neural Networks?&lt;/a&gt;&lt;/cite&gt;&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/images/2022-07-23/do-transformers-see-like-cnns-figure-3.png&quot; alt=&quot;ViT attention span in lower vs., higher layers&quot; /&gt;&lt;figcaption&gt;
      Lower layers in vision transformers pay attention locally and globally – &lt;a href=&quot;https://arxiv.org/abs/2108.08810&quot;&gt;Do Vision Transformers See Like Convolutional Neural Networks?&lt;/a&gt;

    &lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;what-was-not-covered-here&quot;&gt;What was not covered here&lt;/h2&gt;

&lt;p&gt;Vision transformers are barely a few years old. We are still learning more about how to train them and how they behave. This is a short list of active research areas.&lt;/p&gt;

&lt;h3 id=&quot;more-efficient-training-and-inference&quot;&gt;More efficient training and inference&lt;/h3&gt;

&lt;p&gt;Networks with fewer priors embedded in their design need more data to eventually learn these priors that they don’t have. ViT was trained in a dataset of 300 million images. Large datasets are still private (for the most part) and require a huge amount of computer power to train the model.&lt;/p&gt;

&lt;p&gt;New training methods, such as &lt;a href=&quot;https://arxiv.org/abs/2012.12877&quot;&gt;data-efficient image transformers (DeiT)&lt;/a&gt; manage to train vision transformers using only ImageNet (while still large, it’s within reach of more research teams and organizations). See &lt;a href=&quot;https://arxiv.org/abs/2009.06732&quot;&gt;Efficient Transformers: a Survey&lt;/a&gt; for more work in this area.&lt;/p&gt;

&lt;h3 id=&quot;is-attention-needed&quot;&gt;Is “attention” needed?&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;“Attention” is a central concept in transformer networks&lt;/a&gt;. But is it really necessary to achieve the same results? Some intriguing research questions if we need attention at all.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2105.03824&quot;&gt;FNet: Mixing Tokens with Fourier Transforms&lt;/a&gt;: “We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that “mix” input tokens. … [W]e find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths.”&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2105.01601&quot;&gt;MLP-Mixer: An all-MLP Architecture for Vision&lt;/a&gt;  “[W]e show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs) … When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models.”&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;catching-up-with-recent-developments&quot;&gt;Catching up with recent developments&lt;/h2&gt;

&lt;p&gt;Transformers are moving fast. These are some places I use to keep up with recent developments.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Yanni Kilchner reviews recent papers on his &lt;a href=&quot;https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew&quot;&gt;YouTube channel&lt;/a&gt;. It is a great place to go after reading a paper to check your understanding and insights you may have missed on a first pass.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/c/AICoffeeBreak&quot;&gt;AI Coffe Break with Letitia&lt;/a&gt; distills papers into short videos (about ten minutes or so). It’s the ideal format to review the essence of papers.&lt;/li&gt;
  &lt;li&gt;For a slower pace but a broader view, the authors of &lt;a href=&quot;https://arxiv.org/abs/2012.12556&quot;&gt;A Survey on Vision Transformer&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2101.01169&quot;&gt;Transformers in Vision: A Survey&lt;/a&gt; publish new versions of their papers every few months.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Christian Garbin</name></author><category term="machine-learning" /><category term="computer-vision" /><category term="transformers" /><summary type="html">Vision transformers are not just a replacement for CNNs and RNNs. They have some interesting properties.</summary></entry><entry><title type="html">Understanding transformers in one morning</title><link href="https://cgarbin.github.io/understanding-transformers-in-one-morning/" rel="alternate" type="text/html" title="Understanding transformers in one morning" /><published>2022-07-22T00:00:00-04:00</published><updated>2022-07-22T00:00:00-04:00</updated><id>https://cgarbin.github.io/understanding-transformers-in-one-morning</id><content type="html" xml:base="https://cgarbin.github.io/understanding-transformers-in-one-morning/">&lt;p&gt;Transformers are (deservedly so) a hot topic in machine learning.&lt;/p&gt;

&lt;p&gt;If you are new to transformers, the resources in this article will help you understand their fundamentals and applications. It will take about one morning (four hours, give or take) to go through all items.&lt;/p&gt;

&lt;p&gt;I created the list after spending much longer than one morning wading through a large number of articles and videos. I lost time going around in circles, wasting time with superficial sources, or stumbling in articles that were too deep for my level when I first encountered them but were great once I was more prepared.&lt;/p&gt;

&lt;p&gt;This list is organized in a logical sequence, building up the knowledge from the first principles, then going deeper into the details. They are the videos and articles that helped me the most. I hope they help you as well.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;hour-1---the-paper&quot;&gt;Hour 1 - The paper&lt;/h2&gt;

&lt;p&gt;First, read Google AI Research’s blog post &lt;a href=&quot;https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;&gt;Google AI Blog: Transformer: A Novel Neural Network Architecture for Language Understanding&lt;/a&gt;. Don’t follow the links; just read the post. Then read the paper &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention Is All You Need&lt;/a&gt;. Don’t worry about understanding the details at this point. Get familiar with terminology and pictures.&lt;/p&gt;

&lt;p&gt;The paper has about 6,000 words. It would take twenty minutes to read at the average reading pace of 300 words per minute. But it’s a scientific paper, so it will take longer. Using the &lt;a href=&quot;https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf&quot;&gt;three-pass approach&lt;/a&gt;, let’s reserve an hour to read it.&lt;/p&gt;

&lt;h2 id=&quot;hour-2---key-concepts&quot;&gt;Hour 2 - Key concepts&lt;/h2&gt;

&lt;p&gt;The second hour is about understanding the key concepts in the paper with Rasa’s Algorithm Whiteboard video series.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=yGTUuEx3GkA&amp;amp;t=4s&quot;&gt;Rasa Algorithm Whiteboard - Transformers &amp;amp; Attention 1: Self Attention &lt;i class=&quot;fab fa-youtube&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;(14 minutes): Explains &lt;strong&gt;attention&lt;/strong&gt; first with a simple example using a time series, then with a text example. The video introduces &lt;strong&gt;word embedding&lt;/strong&gt;, a key concept for NLP (natural language processing) models, including transformers. With these concepts explained, it defines &lt;strong&gt;self-attention&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=tIvKXrEDMhk&quot;&gt;Rasa Algorithm Whiteboard - Transformers &amp;amp; Attention 2: Keys, Values, Queries  &lt;i class=&quot;fab fa-youtube&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;(13 minutes): Building on the previous video, it explains &lt;strong&gt;keys, queries, and values&lt;/strong&gt;.  First, it explains the operations that make up the &lt;strong&gt;attention layer&lt;/strong&gt; conceptually, as a process to add context to a value (you can think of a “value” as a “word” in this context). Since we are trying to create a model, it describes where we need to add trainable parameters (weights). With the concepts and weights in place, it reintroduces the operations as matrix operations that create the stackable &lt;strong&gt;self-attention block&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=23XUv0T9L5c&quot;&gt;Rasa Algorithm Whiteboard - Transformers &amp;amp; Attention 3: Multi Head Attention &lt;i class=&quot;fab fa-youtube&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt; (11 minutes): Using a phrase as an example, it explains why we need more than one attention head to understand the context where words are used (&lt;strong&gt;multi-head attention&lt;/strong&gt;). The fact that the attention heads are independent is a crucial concept in transformers. It allows matrix operations for each head to run in parallel, significantly speeding up the training process.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=EXNBy8G43MM&quot;&gt;Rasa Algorithm Whiteboard: Transformers &amp;amp; Attention 4 - Transformers &lt;i class=&quot;fab fa-youtube&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;(15 minutes):  With the foundational concepts explained, this video covers the pictures in the “Attention is All You Need” paper that make up the &lt;strong&gt;transformer architecture&lt;/strong&gt;. The new concept introduced here is &lt;strong&gt;positional encoding&lt;/strong&gt;. It ends by highlighting how the transformer architecture lends itself to &lt;strong&gt;parallelization&lt;/strong&gt; in ways other attention architectures cannot.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We just finished the second hour of the morning understanding transformers. Rasa’s videos are a great introduction but are still informal. That’s not a bug. It’s a feature. They introduce the key concepts in simple terms, making them easy to follow.&lt;/p&gt;

&lt;h2 id=&quot;hour-3---digging-into-details&quot;&gt;Hour 3 - Digging into details&lt;/h2&gt;

&lt;p&gt;Now we will switch to a more formal introduction with these two lectures from professor &lt;a href=&quot;https://peterbloem.nl/&quot;&gt;Peter Bloem&lt;/a&gt;, VU University in Amsterdam.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=KmAISyVvE1Y&amp;amp;list=PLIXJ-Sacf8u60G1TwcznBmK6rEL3gmZmV&amp;amp;index=2&quot;&gt;Lecture 12.1 Self-attention &lt;i class=&quot;fab fa-youtube&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;(23 minutes): Explains, with the help of illustrations, the matrix operations to calculate self-attention, then moves on to keys, queries, and values. With the basic concepts in place, it explains why we need multi-head attention.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=oUhGZMCTHtI&amp;amp;list=PLIXJ-Sacf8u60G1TwcznBmK6rEL3gmZmV&amp;amp;index=3&quot;&gt;Lecture 12.2 Transformers &lt;i class=&quot;fab fa-youtube&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt;(18 minutes): Examines the pieces that make up the transformer model in the paper. The pictures from the paper are dissected with some math and code.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;hour-4---pick-your-adventure&quot;&gt;Hour 4 - Pick your adventure&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Go wide with &lt;a href=&quot;https://sea-adl.org/2019/12/03/lstm-is-dead-long-live-transformers/&quot;&gt;LSTM is dead, long live Transformers  &lt;i class=&quot;fab fa-youtube&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt; (30 minutes): This talk gives a sense of history, explaining how we approached natural language problems in the past, their limitations, and how transformers overcame those limitations. It shows how to implement the transformer calculations with Python code. If you are better at visualizing code than math (like me), this can help you understand the operations.&lt;/li&gt;
  &lt;li&gt;Go deep with &lt;a href=&quot;http://nlp.seas.harvard.edu/annotated-transformer/&quot;&gt;The Annotated Transformer&lt;/a&gt; (30 to 60 minutes to read, hours and hours to experiment):  This article by the Harvard NLP team annotates the transformer paper with modern (as of 2022) PyTorch code. Each section of the paper is supplemented by the code that implements it. Part 3, “A Real World Example”, implements a fully functional German-English translation example using a &lt;a href=&quot;https://torchtext.readthedocs.io/en/latest/datasets.html#multi30k&quot;&gt;smaller dataset&lt;/a&gt; that makes it workable on smaller machines.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;where-to-go-from-here&quot;&gt;Where to go from here&lt;/h2&gt;

&lt;p&gt;It is a good time to reread the paper. It will make more sense now.&lt;/p&gt;

&lt;p&gt;These are other articles and videos that helped me understand transformers. Some of them overlap with the ones above, and some are complementary.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Positional embedding (encoding) is a key concept in understanding transformers. The transformer paper assumes that the reader knows that concept and briefly explains the reasons to use sine and cosine. &lt;a href=&quot;https://www.youtube.com/watch?v=1biZfFLPRSY&quot;&gt;This video  &lt;i class=&quot;fab fa-youtube&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt; from &lt;em&gt;AI Coffee Break with Letitia&lt;/em&gt; explains in under ten minutes the concepts and the reasons to use sine and cosine.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://peterbloem.nl/blog/transformers&quot;&gt;Transformers from scratch&lt;/a&gt; is the accompanying blog post to hour 3, “Digging into details.” Professor Bloem describes some concepts explored in the video and adds code to show they are implemented.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://e2eml.school/transformers.html&quot;&gt;Transformers from Scratch&lt;/a&gt; (same title, different article) takes more time than other articles to explain one-hot encoding, dot product, and matrix multiplication, among others, with illustrations. By the time it gets to “attention as matrix multiplication”, it’s easier to understand the math. This post can be a good refresher if you are rusty on the math side of machine learning.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tensorflow.org/text/tutorials/transformer&quot;&gt;Transformer model for language understanding&lt;/a&gt; is TensorFlow’s official implementation of the paper. It is not as annotated as the PyTorch code in &lt;a href=&quot;http://nlp.seas.harvard.edu/annotated-transformer/&quot;&gt;The Annotated Transformer&lt;/a&gt;, but still helpful if you are in a TensorFlow shop.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://johnthickstun.com/docs/transformers.pdf&quot;&gt;The Transformer Model in Equations&lt;/a&gt; is exactly what the name says, transformers as mathematical operations. The “Discussion” section is an insightful explanation of the equations, valuable even if you don’t have a strong math background (like me).&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;The Illustrated Transformer&lt;/a&gt; is an often-cited source for understanding transformers. It is a good source if someone has time to read only one article beyond the paper.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a sense of history, these are two papers highly cited as works that led us to the transformer architecture.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt; is the paper credited with introducing the “attention” mechanism.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1508.04025&quot;&gt;Effective Approaches to Attention-based Neural Machine Translation&lt;/a&gt; builds on the previous paper, introducing other important concepts, including dot-product attention. This &lt;a href=&quot;https://www.tensorflow.org/text/tutorials/nmt_with_attention&quot;&gt;official Tensorflow notebook&lt;/a&gt; implements a Spanish-to-English translation based on the paper.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, &lt;a href=&quot;https://www.youtube.com/watch?v=rBCqOTEfxvg&quot;&gt;Attention is all you need; Attentional Neural Network Models &lt;i class=&quot;fab fa-youtube&quot; aria-hidden=&quot;true&quot;&gt;&lt;/i&gt;&lt;/a&gt; is a talk by Łukasz Kaiser, one of the &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;paper’s&lt;/a&gt; authors. He builds up the solution, starting with how natural language translation used to be solved in the past, the limitations, and how transformers solve them. So far, it’s what I would expect from one of the authors. What makes this video interesting to me is how humble Łukasz is. He explains the trials and errors and, at one point, how they had to ask for help to train the model they created.&lt;/p&gt;

&lt;p&gt;Reading a scientific paper makes it look like a linear story from problem to solution (“we had an idea and implemented it”). Watching Łukasz talk helps understand how these great solutions don’t arrive out of thin air. Researchers build on top of previous work, try many variations, make mistakes, and ask for help to complete their work. Then they write the paper…&lt;/p&gt;

&lt;hr /&gt;

&lt;p class=&quot;small&quot;&gt;If your interests are in computer vision, &lt;a href=&quot;/vision-transformers-properties/&quot;&gt;it turns out transformers work quite well for that too&lt;/a&gt;.&lt;/p&gt;</content><author><name>Christian Garbin</name></author><category term="machine-learning" /><category term="natural-language-processing" /><category term="nlp" /><category term="transformers" /><summary type="html">Transformers: from zero to hero in one morning (or at least know enough to discuss transformers intelligently and apply them to your projects).</summary></entry><entry><title type="html">Applications of transformers in computer vision</title><link href="https://cgarbin.github.io/transformers-in-computer-vision/" rel="alternate" type="text/html" title="Applications of transformers in computer vision" /><published>2021-12-01T00:00:00-05:00</published><updated>2021-12-01T00:00:00-05:00</updated><id>https://cgarbin.github.io/transformers-in-computer-vision</id><content type="html" xml:base="https://cgarbin.github.io/transformers-in-computer-vision/">&lt;p&gt;This article describes the evolution of transformers, their application in natural language processing (NLP), their surprising effectiveness in computer vision, ending with applications in healthcare.&lt;/p&gt;

&lt;p&gt;It starts with the motivation and origins of transformers, from the initial attempts to apply a specialized neural network architecture (recurrent neural network – RNN) to natural language processing (NLP), the evolution of such architectures (long short-term memory and the concept of attention), to the creation of transformers and what makes them perform well in NLP. Then it describes how transformers are applied to computer vision. The last section describes some of the applications of transformers in healthcare (an area of interest for my research).&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Side note: It was originally written as a survey paper for a class I took. Hence the references are in bibliography format instead of embedded links.&lt;/p&gt;

&lt;p class=&quot;notice--info&quot;&gt;if you are new to transformers, see &lt;a href=&quot;/understanding-transformers-in-one-morning/&quot;&gt;Understanding transformers in one morning&lt;/a&gt; and &lt;a href=&quot;/vision-transformers-properties/&quot;&gt;Vision transformer properties&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;the-origins-of-transformers--natural-language-processing&quot;&gt;The origins of transformers – natural language processing&lt;/h1&gt;

&lt;h2 id=&quot;when-context-matters&quot;&gt;When context matters&lt;/h2&gt;

&lt;p&gt;In some machine learning applications, we train models by feeding one input at a time. The trained model is then used in the same way: given one input, make a prediction. The typical example is image recognition and classification. We train the model by feeding one image at a time. Once trained, we feed one image and the model returns a prediction.&lt;/p&gt;

&lt;p&gt;However, there are other classes of problems where a single input is not enough to make a prediction. Natural language processing is a prominent example. When translating a sentence, it is not enough to look at one word at a time. The context in which a word is used matters. For example, the Portuguese word legal is translated in different ways to English.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Isso é um argumento &lt;strong&gt;legal&lt;/strong&gt; → This is a &lt;strong&gt;legal&lt;/strong&gt; argument&lt;/p&gt;

  &lt;p&gt;Isso é um seriado &lt;strong&gt;legal&lt;/strong&gt; → This is a &lt;strong&gt;nice&lt;/strong&gt; TV series&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In these applications of machine learning, context matters. The translation of “legal” depends on the word that came before it. If we represent the phrases as vectors (so a model can process them), we could, for example, represent the first phrase as the vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p1=[87,12,43,215,102]&lt;/code&gt; and the second sentence as the vector &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p2=[87,12,43,175,102]&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;A model attempting to translate the word “legal”, encoded as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;102&lt;/code&gt;, must remember what came before it. The model must translate &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;102&lt;/code&gt; one way if it was preceded by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;215&lt;/code&gt; (p1) and another way if it was preceded by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;175&lt;/code&gt; (p2).&lt;/p&gt;

&lt;p&gt;The model must have a “memory” of what it has seen so far. Or, in other words, the model’s output is contextual: it is based not only on its current state (the current input – the current word) but also on previous states (what came before the current input – the words that came before). To understand the context, the model must “remember” what it has seen so far, instead of taking only one input at a time, i.e. the model must work with a sequence of input values.&lt;/p&gt;

&lt;h2 id=&quot;remembering-the-past--recurrent-neural-networks&quot;&gt;Remembering the past – recurrent neural networks&lt;/h2&gt;

&lt;p&gt;Recurrent neural networks (RNNs) are a class of networks that can model such problems. The figure below shows the standard representation of an RNN cell. The blue arrow indicates the “temporal loop” in the network: the result from a previous input, known as the state, is fed into the network when processing a new input. Using the state from a previous input when processing new input allows the network to “remember” what it has seen so far.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-12-01/rnn-one-cell.png&quot; alt=&quot;One cell of an RNN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The temporal loop can be conceptually represented as passing the state from the past steps into the future steps. In the figure below, the RNN cell is unrolled (repeated) to represent the state from previous steps passed into the subsequent ones (this process is also called “unfolding” the network).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-12-01/rnn-unrolled.png&quot; alt=&quot;Unrolled RNN&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;forgetting-the-past--vanishing-and-exploding-gradients&quot;&gt;Forgetting the past – vanishing and exploding gradients&lt;/h2&gt;

&lt;p&gt;RNNs are trained with a variation of back-propagation, similar to how we train other types of neural networks. First, we choose how many steps we will unroll the network, and then we apply a specialized version of back-propagation (Ian et al., 2016).&lt;/p&gt;

&lt;p&gt;Ideally, we would like to create an RNN with as many unrolled steps as possible, to have as much context as possible (i.e. remember very large sentences or even entire pieces of text). However, a large number of unrolled steps has an unfortunate effect: vanishing and exploding gradients, which limits the size of the network we can build (Bengio et al., 1994) (Pascanu et al., 2013).&lt;/p&gt;

&lt;p&gt;In practice, the result is that we have to limit the number of unrolled steps of an RNN, thus limiting how far back the network can “remember” information.&lt;/p&gt;

&lt;h2 id=&quot;going-further-into-the-past--long-short-term-memory&quot;&gt;Going further into the past – long short-term memory&lt;/h2&gt;

&lt;p&gt;Long short-term memory (LSTM) is a recurrent network architecture created to deal with the vanishing and exploding gradient problem of the classical RNN architecture (Hochreiter &amp;amp; Schmidhuber, 1997). They do so by having a more complex cell design. In this design, the gradients are all contained within the LSTM cell, making them more stable because they no longer have to traverse the entire network.&lt;/p&gt;

&lt;p&gt;The figure below, from (Greff et al., 2017), compares an RNN cell (left) with a typical LSTM cell (right), including the “forget gate” that enables it to learn long sequences that are not partitioned into subsequences (Gers et al., 1999).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-12-01/rnn-lstm-compared.png&quot; alt=&quot;RNN and LSTM compared&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;deciding-where-to-look--attention&quot;&gt;Deciding where to look – attention&lt;/h2&gt;

&lt;p&gt;With LSTM we have a solution to look further into the past and process larger sentences. Now we need to decide where to look when processing a sentence because the order of the words is important for language processing. A model cannot mindlessly translate one word at a time.&lt;/p&gt;

&lt;p&gt;A typical example where the order of words matters is the placement of adjectives. Back to the first example, we can see that the placement of “legal” varies in each language.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Isso é um argumento &lt;strong&gt;legal&lt;/strong&gt; → This is a &lt;strong&gt;legal&lt;/strong&gt; argument&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;How does a model know that “legal” goes to a different position in the translated phrase? The solution has two parts. First, the model needs to process the entire sentence, not each word separately. Then, the model needs to learn that it has to pay more attention to some parts of the phrases than others, at different times (in the example above, although “legal” comes last in the input, the model has to learn that in the output it must come first).&lt;/p&gt;

&lt;p&gt;RNN encoder/decoder networks (Cho et al., 2014) are used for the first part, processing the entire sentence. An encoder/decoder has two neural networks: one that converts (encodes) a sequence of words into a representation suitable to train a network, and another network that takes the encoded representation and translates (decodes) it. The decoder, armed with a full sequence of words and not just one word, implements the second part of the solution: decide in which sequence it must process the words (which may not be in the same order they were received, as in this case).&lt;/p&gt;

&lt;p&gt;This process is known as &lt;em&gt;attention&lt;/em&gt; (Bahdanau et al., 2014) (Luong et al., 2015), as in “where should the decoder look to produce the next output”.&lt;/p&gt;

&lt;h2 id=&quot;attention-is-all-we-need--transformers&quot;&gt;“Attention is all we need” – transformers&lt;/h2&gt;

&lt;p&gt;Adding the concept of attention significantly improved the accuracy of the networks, but it is still part of a time-consuming process, the training of the encoder and decoder RNNs.&lt;/p&gt;

&lt;p&gt;If what we want is the information to calculate attention, can we do that in a faster way? It turns out we can. Transformer networks dispense with RNNs and directly compute the important piece of information we want, attention. They achieve better accuracy for a fraction of the training time (Jakob, 2017) (Vaswani et al., 2017).&lt;/p&gt;

&lt;p&gt;Instead of using RNNs, transformers use stacks of feed-forward layers (a simple layer of neurons, without cycles, unlike RNNs). The figure below, from the original paper (Vaswani et al., 2017), shows the network architecture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-12-01/transformer-model-architecture.png&quot; alt=&quot;Transformer model architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Dispensing with RNNs has two effects: the training process can be parallelized (RNNs are sequential by definition: the state of a previous step is fed into the next step) and computations are much faster. The following table, from (Vaswani et al., 2017), shows the smaller computational complexity of the transformer model compared to RNNs and convolutional neural networks (CNNs).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-12-01/transformer-computational-complexity.png&quot; alt=&quot;Transformer computational complexity&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The rightmost columns of the following table, also from (Vaswani et al., 2017), compares the training cost (in FLOPs). The transformer models are two to three orders the magnitude less expensive to train.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-12-01/transformer-training-cost.png&quot; alt=&quot;Transformer training cost&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The best performing language models today, BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), and GPT-3 (Brown et al., 2020), are based on the transformer architecture. The combination of a simpler network and parallelization allowed the creation of these large, sophisticated models.&lt;/p&gt;

&lt;p&gt;A key concept of the transformer architecture is the “multi-head self-attention” layer. “Multi”  refers to the fact that instead of having one attention layer, transformers have multiple attention layers running in parallel. In addition, the layers employ self-attention (Cheng et al., 2016) (Lin et al., 2017). With such a construct, transformers can efficiently weigh in the contribution of multiple parts of a sentence simultaneously. Each self-attention layer can encode longer range dependencies, capturing the relationship between words that are further apart (compared to RNNs and CNNs).&lt;/p&gt;

&lt;p&gt;The ability to pay attention to multiple parts of the input and the encoding of longer-range dependencies results in better accuracy. The figure below (Alammar, 2018b) shows how self-attention allows a model to learn that “it” refers more strongly to “The animal” in the sentence.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-12-01/transformer-attention-example.png&quot; alt=&quot;Transformer attention example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Research continues to create larger transformer models. A recent advancement in the architecture of transformers is Big Bird (Zaheer et al., 2021). It removes the original model’s quadratic computational and memory dependency on the sequence length by introducing sparse attention. By removing the quadratic dependency, larger models can be built, capable of processing larger sequences.&lt;/p&gt;

&lt;h1 id=&quot;transformers-in-computer-vision&quot;&gt;Transformers in computer vision&lt;/h1&gt;

&lt;p&gt;The concepts of “sequence” and “attention” can also be applied to computer vision. The original applications of attention in image processing used RNNs, like the NLP counterparts. Neural networks with attention were used for image classification (Mnih et al., 2014), multiple object recognition (Ba et al., 2015), and image caption generation (Xu et al., 2016). These applications of attention to computer vision experienced the same issues that afflicted NLP architectures based on RNN: vanishing or exploding gradients and long times to train the model.&lt;/p&gt;

&lt;p&gt;And, just like in NLP, the solution was to apply self-attention, using the transformer architecture. One of the first applications of transformers in computer vision was in image generation (Parmar et al., 2018). (Carion et al., 2020) applied transformers to object detection and segmentation using a hybrid architecture, with a CNN used to extract image features.&lt;/p&gt;

&lt;p&gt;Then (Dosovitskiy et al., 2020, which includes references to earlier works they built upon), dropped all other types of networks, creating a “pure” transformer architecture for image recognition. In the figure below, from that paper, we can see the same elements of the NLP transformer architecture, now applied to computer vision: the lack of more complex networks (like RNN or CNN) that results in fast training time, the concept of sequences (created by splitting the image into patches), and the multi-headed attention. This architecture is known as ViT (Vision Transformer).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-12-01/vision-transformer-architecture.png&quot; alt=&quot;Vision Transformer architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The resulting transformer models are more accurate than the convolutional neural network (CNN) models typically used in computer vision and, more importantly, significantly faster to train. In the table below, from (Dosovitskiy et al., 2020), the first three columns are three versions of the transformer model. The last row shows how the transformer-based networks (first three columns) use substantially less computational resources for training than CNN-based networks (last two columns).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-12-01/vision-transformer-performance.png&quot; alt=&quot;Vision Transformer performance&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Transformers in computer vision is still an active area of research. At the time of this writing (November of 2021), the recently-published Swin Transformer architecture (Liu et al., 2021) used a shifted windows approach (figure below, from the paper) to achieve state-of-the-art results in image classification, object detection, and image segmentation. The shifted window architecture allows a transformer network to cope with the “…large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text.”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-12-01/swin-transformer.png&quot; alt=&quot;Swin Transformer&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;transformers-in-healthcare&quot;&gt;Transformers in healthcare&lt;/h1&gt;

&lt;p&gt;Applications of Transformers in healthcare fall, in general terms, into the following categories:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Natural language process (NLP)&lt;/em&gt;: extract information from medical records to make predictions.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Genomics and proteomics&lt;/em&gt;:  processing the large sequences from genetic and proteomic.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Computer vision&lt;/em&gt;: image classification, segmentation, augmentation, and generation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following sections describe some of these applications. Note from the dates of the references that this is a recent and active area of research. Many of the current applications of CNNs and RNNs in the same areas have evolved over the years until they reached their current performance. It is expected that these early (and promising) applications of transformers will improve over time as research continues.&lt;/p&gt;

&lt;h2 id=&quot;nlp-applications&quot;&gt;NLP applications&lt;/h2&gt;

&lt;p&gt;The healthcare industry has been accumulating written records for many years. There is a wealth of information stored in these records from consultation notes, lab exam summaries, and radiologists’ reports. Most of them are already stored in electronic health records (EHR), ready to be consumed by computers. Transformers’ success with NLP makes them a good fit to process EHR. Some of the applications include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BEHRT (Li et al., 2020), as the name indicates, was inspired by BERT (Devlin et al., 2019). Trained on medical records, BEHRT can predict 301 diseases in a future visit of a patient. It improved the state-of-the-art in this task by “8.0–13.2% (in terms of average precision scores for different tasks)”. In addition to the improvements in prediction, the attention mechanism has the potential to make the model more interpretable, an important feature for healthcare applications.&lt;/li&gt;
  &lt;li&gt;(Kodialam et al., 2020) introduces SARD (self-attention with reverse distillation), where the input to the model is not the raw text from medical records but a summary of a medical visit. While BEHRT can handle 301 conditions, SARD can handle “…a much larger set of 37,004 codes, spanning conditions, medications, procedures, and physician specialty.”&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;genomics-and-proteomics-applications&quot;&gt;Genomics and proteomics applications&lt;/h2&gt;

&lt;p&gt;Transformers’ ability to process sequences makes them natural candidates for genomics and proteomics applications, where large, complex sequences abound.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;AlphaFold2 (Jumper et al., 2021) is an evolution of the first AlphaFold. It decisively won the 14th Critical Assessment of Structural Prediction (CASP), a competition to predict the structure (“folds”) of proteins. Understanding the structure of proteins is important because the function of a protein is directly related to its structure. Given that the structure of a protein is determined by its amino acid sequence, it is not surprising to learn that one of the most important changes in AlphaFold2 was the addition of attention via transformers (Rubiera, 2021). AlphaFold2’s transformer architecture has been named EvoFormer.&lt;/li&gt;
  &lt;li&gt;(Avsec et al., 2021) applied transformers to gene expression. They named the architecture Enformer (“a portmanteau of enhancer and transformer”). Gene expression is a fundamental building block in biology. It is “the process by which information from a gene is used in the synthesis of a functional gene product that enables it to produce end products, protein or non-coding RNA, and ultimately affect a phenotype, as the final effect.” (Wikipedia, 2021).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these applications in mind, the figure below (Avsec, 2021) illustrates why the ability to process larger sequences makes transformers an effective architecture for genomics and proteomics applications. The dark blue area shows how far the Enformer architecture can look for interactions between DNA base pairs (200,000), compared with the previous state-of-the-art Basenji2 architecture (40,000 base pairs).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-12-01/enformer.png&quot; alt=&quot;Enformer&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;computer-vision-applications&quot;&gt;Computer vision applications&lt;/h2&gt;

&lt;p&gt;Transformers are improving the following areas of healthcare computer vision:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Label generation&lt;/em&gt;: extract accurate labels from medical records to train image classification networks.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Large image analysis&lt;/em&gt;: process the large images generated in some medical areas.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Improvements to explainability&lt;/em&gt;: produce explanations that are easier to interpret for medical professionals.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following sections expand on those areas.&lt;/p&gt;

&lt;h3 id=&quot;label-generation&quot;&gt;Label generation&lt;/h3&gt;

&lt;p&gt;Medical image applications that identify diseases and other features in images are trained with supervised or semi-supervised learning, which means they need many images with accurate labels. Labeling medical images requires experts that are few, expensive, or both.&lt;/p&gt;

&lt;p&gt;On the other hand, there are many images with accompanying medical reports, for example, the radiological reports from x-rays. An application capable of reliably extracting labels from the reports can boost the number of images in medical image datasets. However, medical reports are created by human experts for other human experts. The reports contain complex sentences that record not only the expert’s certainty about findings but also other potential findings and exclusions. Telling apart positive, potential, and negated (excluded) findings is a complex task.&lt;/p&gt;

&lt;p&gt;CheXpert (Irvin et al., 2019) made available over 100,000 chest x-ray images with labels extracted from the medical reports using a rule-based NLP parser. The same team later developed ChexBert (Smit et al., 2020) based on (as the name implies) BERT (Devlin et al., 2019). CheXBert performed better than CheXPert and, crucially, it performs better in uncertainty (“potential”, “unremarkable”, and similar words) and negation cases, which are notoriously difficult to analyze.&lt;/p&gt;

&lt;p&gt;These results indicate that transformer-based labeling extraction can improve the labels of existing datasets and help create more trustworthy labeled medical images, which are necessary to advance research in healthcare computer vision.&lt;/p&gt;

&lt;h3 id=&quot;large-image-analysis&quot;&gt;Large image analysis&lt;/h3&gt;

&lt;p&gt;Some medical diagnosis images, such as those used in histopathology, are large, in the hundreds of megabytes to the gigapixel range. Traditional neural networks cannot handle such images in one piece. Before transformers, a common solution was to split the image into multiple patches and process them separately with a CNN-based network  (Komura &amp;amp; Ishikawa, 2018). Dividing an image into arbitrary patches may lose context information about the overall image structure and features.&lt;/p&gt;

&lt;p&gt;Holistic Attention Network – HATNet (Mehta et al., 2020) is a transformer-based architecture that takes a different approach, borrowing concepts from NLP. Instead of analyzing each patch separately, it considers each patch a “word” and combines them into bags of words. The bags of words are then processed by a transformer network that aggregates information from the different patches into a global image representation. HATNet is “8% more accurate and about 2× faster than the previous best network”.&lt;/p&gt;

&lt;p&gt;More important than the immediate results of HATNet is the innovative approach that opens up the door to more research into processing large medical images. For example, TransUNet (Chen et al., 2021) takes a similar approach for medical image segmentation. As in image classification, CNNs have been traditionally applied to medical image segmentation. Using CNNs for segmentation has a related problem as for classification: the CNNs lose global context. TransUNet resolves that problem with a hybrid architecture: a CNN is used to extract features from the large-dimensional images, which are then passed to a  transformer network. It improved the state-of-the-art Synapse multi-organ CT segmentation by several percentage points.&lt;/p&gt;

&lt;h3 id=&quot;improvements-in-interpretability&quot;&gt;Improvements in interpretability&lt;/h3&gt;

&lt;p&gt;In high-stakes applications, such as healthcare, interpretable results help improve “auditability, system verification, enhance trust, and user adoption” (Reyes et al., 2020). Specifically for medical images, interpretability is related to explaining what pieces of an image the model considered for inference.&lt;/p&gt;

&lt;p&gt;Although still a new field, the interpretability of image classification with transformers shows early signs that it can result in more precise, and thus more helpful, interpretations of what a model is “looking” at. In the figure below, from (Chefer et al., 2021), the rightmost column shows their new method to extract interpretability from a transformer multi-class image classification task. It generates class-specific visualizations with better-defined activations. The closest alternative method is Grad-CAM (Selvaraju et al., 2020) (other methods cannot even generate class-specific visualizations), but it has significantly more extraneous artifacts in the visualization.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-12-01/interpretability.png&quot; alt=&quot;Interpretability&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The transformer’s attention map also shows promising results for interpretability. In the figure below, from (Matsoukas et al., 2021), the top row shows the original image of a dermoscopic image (left), an eye fundus (center), and a mammography (right). The middle row is a Grad-CAM saliency map, traditionally used to interpret the classification from CNNs. The bottom row is a saliency map from a transformer attention layer. The attention layer saliency shows a more well-defined saliency area, making the results easier to interpret (although the paper notes that this assumption has to be tested with medical professionals).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-12-01/saliency-maps.png&quot; alt=&quot;Interpretability&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h1&gt;

&lt;p&gt;Transformers were first used in NLP applications, resulting in impressive language models like BERT, GPT-2, and GTP-3. Their ability to learn the association between pieces of a large sequence of data (attention) is now being used in computer vision. The resulting models are faster to train and more accurate than CNNs for image classification.&lt;/p&gt;

&lt;p&gt;From the literature references, we notice that applying transformers to computer vision is still a new area. CNN- and RNN-based solutions evolved over many years of research. We should expect transformers also to evolve. In fact,  several approaches are already being tried to create more efficient transformer architectures by, for example, reducing the quadratic complexity of the attention mechanism (May, 2020), (Tay et al., 2020), (Choromanski &amp;amp; Colwell, 2020).&lt;/p&gt;

&lt;p&gt;Efficient transformer architectures will have two effects. From one side, larger and larger sequences will be handled, improving the results in applications where the size of the sequence is critical for the results (for example, large resolution images used in healthcare). On the other hand, for the same sequence length, it will become faster, and thus cheaper, to train transformers, democratizing their use.&lt;/p&gt;

&lt;p&gt;And, as a final benefit, we may end up with one unified network architecture that can be applied to two important fields, natural language processing, and computer vision.&lt;/p&gt;

&lt;h1 id=&quot;appendix-a---a-reading-list-for-rnn-lstm-attention-and-transformers-in-nlp&quot;&gt;Appendix A - A reading list for RNN, LSTM, attention, and transformers in NLP&lt;/h1&gt;

&lt;p&gt;While researching this paper, I started with the original application of the networks, natural language processing (NLP). After researching the applications for image processing, it became clear that starting with NLP was indeed a good choice. The concepts of sequence and attention are easier to illustrate and follow in that area. Once learned in that context, they can be transferred to computer vision.&lt;/p&gt;

&lt;p&gt;This appending is a reading list in the context of NLP to help other readers, and the future self of the author when he will (inevitably) have forgotten some of the concepts.&lt;/p&gt;

&lt;p&gt;The seminal paper on encoder/decoder combined with RNN for natural language processing is (Cho et al., 2014).  (Sutskever et al., 2014) introduced sequence-to-sequence using long short-term memory (LSTM) networks. (Bahdanau et al., 2014) and (Luong et al., 2015) are credited with developing the attention mechanism.  (Vaswani et al., 2017) is the original paper on transformers (reading the accompanying Google’s blog post (Jakob, 2017) makes it easier to follow the paper).&lt;/p&gt;

&lt;p&gt;The explanations of RNN and LSTM in this paper are simplified because I wanted to focus on transformers. I did not discuss the different types of RNNs and the inner working of the LSTM cell. For a step-by-step, illustrated explanation of how LSTMs work and why it is an effective RNN architecture, see (Olah, 2015). For other RNN architectures, see (Olah &amp;amp; Carter, 2016).&lt;/p&gt;

&lt;p&gt;(Alammar, 2018a) describes step-by-step, with the help of animated visualizations the sequence-to-sequence, encoder/decoder, RNN, and attention concepts, including details of how they work. (Alammar, 2018b) builds on that to explain how transformers use the important concept of self-attention, with detailed illustrations.&lt;/p&gt;

&lt;p&gt;Finally, as a historical note: finding the original paper on recurrent networks (RNNs) turned out to be elusive. Like many ideas, it evolved over time. (Rumelhart et al., 1987) is credited in several places as the first mention and description of a “recurrent network”, although it did not describe the back-propagation through time (BPTT) method used to train RNNs nowadays.&lt;/p&gt;

&lt;h1 id=&quot;appendix-b---the-quadratic-bottleneck&quot;&gt;Appendix B - The quadratic bottleneck&lt;/h1&gt;

&lt;p&gt;As a general rule, the longer the sequence a transformer can process, the better results it will have. However, it comes at the cost of large amounts of memory and processing power required for training and inference. The self-attention mechanism of the standard transformer architecture is a quadratic function (figure below, from (Tay et al., 2020)).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-12-01/quadratic-problem.png&quot; alt=&quot;The quadratic problem&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Several approaches are being tried to reduce the quadratic complexity, creating more efficient transformer architectures (May, 2020), (Tay et al., 2020), (Choromanski &amp;amp; Colwell, 2020).&lt;/p&gt;

&lt;p&gt;Efficient transformer architectures will have two effects. From one side, larger and larger sequences will be handled, improving the results in applications where the size of the sequence is critical for the results (for example, large resolution images used in healthcare). On the other hand, for the same sequence length, it will become faster, and thus cheaper, to train transformers, democratizing their use.&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;Alammar, J. (2018a, May 9) &lt;a href=&quot;https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&quot;&gt;Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Alammar, J. (2018b, June 27) &lt;a href=&quot;http://jalammar.github.io/illustrated-transformer/&quot;&gt;The Illustrated Transformer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Avsec, Ž. (2021, October 4) &lt;a href=&quot;https://deepmind.com/blog/article/enformer&quot;&gt;Predicting gene expression with AI. Deepmind&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Avsec, Ž., Agarwal, V., Visentin, D., Ledsam, J. R., Grabska-Barwinska, A., Taylor, K. R., Assael, Y., Jumper, J., Kohli, P., &amp;amp; Kelley, D. R. (2021) &lt;a href=&quot;https://doi.org/10.1038/s41592-021-01252-x&quot;&gt;Effective gene expression prediction from sequence by integrating long-range interactions. Nature Methods, 18(10), 1196–1203&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Ba, J., Mnih, V., &amp;amp; Kavukcuoglu, K. (2015) &lt;a href=&quot;http://arxiv.org/abs/1412.7755&quot;&gt;Multiple Object Recognition with Visual Attention&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Bahdanau, D., Cho, K., &amp;amp; Bengio, Y. (2014) &lt;a href=&quot;https://arxiv.org/abs/1409.0473v7&quot;&gt;Neural Machine Translation by Jointly Learning to Align and Translate&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Bengio, Y., Simard, P., &amp;amp; Frasconi, P. (1994) &lt;a href=&quot;https://doi.org/10.1109/72.279181&quot;&gt;Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2), 157–166.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020) &lt;a href=&quot;http://arxiv.org/abs/2005.14165&quot;&gt;Language Models are Few-Shot Learners&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., &amp;amp; Zagoruyko, S. (2020) &lt;a href=&quot;http://arxiv.org/abs/2005.12872&quot;&gt;End-to-End Object Detection with Transformers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Chefer, H., Gur, S., &amp;amp; Wolf, L. (2021) &lt;a href=&quot;http://arxiv.org/abs/2012.09838&quot;&gt;Transformer Interpretability Beyond Attention Visualization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A. L., &amp;amp; Zhou, Y. (2021) &lt;a href=&quot;http://arxiv.org/abs/2102.04306&quot;&gt;TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Cheng, J., Dong, L., &amp;amp; Lapata, M. (2016) &lt;a href=&quot;http://arxiv.org/abs/1601.06733&quot;&gt;Long Short-Term Memory-Networks for Machine Reading&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Cho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp;amp; Bengio, Y. (2014) &lt;a href=&quot;http://arxiv.org/abs/1406.1078&quot;&gt;Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Choromanski, K., &amp;amp; Colwell, L. (2020, October 23) &lt;a href=&quot;http://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html&quot;&gt;Rethinking Attention with Performers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Devlin, J., Chang, M.-W., Lee, K., &amp;amp; Toutanova, K. (2019) &lt;a href=&quot;http://arxiv.org/abs/1810.04805&quot;&gt;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., &amp;amp; Houlsby, N. (2020) &lt;a href=&quot;https://arxiv.org/abs/2010.11929v2&quot;&gt;An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Gers, F. A., Schmidhuber, J., &amp;amp; Cummins, F. (1999) &lt;a href=&quot;https://doi.org/10.1049/cp:19991218&quot;&gt;Learning to forget: Continual prediction with LSTM. 1999 Ninth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470), 2, 850–855 vol.2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Greff, K., Srivastava, R. K., Koutník, J., Steunebrink, B. R., &amp;amp; Schmidhuber, J. (2017) &lt;a href=&quot;https://doi.org/10.1109/TNNLS.2016.2582924&quot;&gt;LSTM: A Search Space Odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28(10), 2222–2232&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Hochreiter, S., &amp;amp; Schmidhuber, J. (1997) &lt;a href=&quot;https://doi.org/10.1162/neco.1997.9.8.1735&quot;&gt;Long Short-Term Memory. Neural Computation, 9(8), 1735–1780&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Ian, G., Yoshua, B., &amp;amp; Aaron, C. (2016) &lt;a href=&quot;https://www.deeplearningbook.org/&quot;&gt;Deep Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Jakob, U. (2017) &lt;a href=&quot;http://ai.googleblog.com/2017/08/transformer-novel-neural-network.html&quot;&gt;Transformer: A Novel Neural Network Architecture for Language Understanding&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Jumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, R., Žídek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S. A. A., Ballard, A. J., Cowie, A., Romera-Paredes, B., Nikolov, S., Jain, R., Adler, J., … Hassabis, D. (2021) &lt;a href=&quot;https://doi.org/10.1038/s41586-021-03819-2&quot;&gt;Highly accurate protein structure prediction with AlphaFold. Nature, 596(7873), 583–589&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Kodialam, R. S., Boiarsky, R., Lim, J., Dixit, N., Sai, A., &amp;amp; Sontag, D. (2020) &lt;a href=&quot;http://arxiv.org/abs/2007.05611&quot;&gt;Deep Contextual Clinical Prediction with Reverse Distillation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Komura, D., &amp;amp; Ishikawa, S. (2018) &lt;a href=&quot;https://doi.org/10.1016/j.csbj.2018.01.001&quot;&gt;Machine Learning Methods for Histopathological Image Analysis. Computational and Structural Biotechnology Journal, 16, 34–42&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Li, Y., Rao, S., Solares, J. R. A., Hassaine, A., Ramakrishnan, R., Canoy, D., Zhu, Y., Rahimi, K., &amp;amp; Salimi-Khorshidi, G. (2020) &lt;a href=&quot;https://doi.org/10.1038/s41598-020-62922-y&quot;&gt;BEHRT: Transformer for Electronic Health Records. Scientific Reports, 10(1), 7155&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Lin, Z., Feng, M., Santos, C. N. dos, Yu, M., Xiang, B., Zhou, B., &amp;amp; Bengio, Y. (2017) &lt;a href=&quot;http://arxiv.org/abs/1703.03130&quot;&gt;A Structured Self-attentive Sentence Embedding&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., &amp;amp; Guo, B. (2021) &lt;a href=&quot;http://arxiv.org/abs/2103.14030&quot;&gt;Swin Transformer: Hierarchical Vision Transformer using Shifted Windows&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Luong, M.-T., Pham, H., &amp;amp; Manning, C. D. (2015) &lt;a href=&quot;http://arxiv.org/abs/1508.04025&quot;&gt;Effective Approaches to Attention-based Neural Machine Translation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Matsoukas, C., Haslum, J. F., Söderberg, M., &amp;amp; Smith, K. (2021) &lt;a href=&quot;http://arxiv.org/abs/2108.09038&quot;&gt;Is it Time to Replace CNNs with Transformers for Medical Images?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;May, M. (2020, March 14) &lt;a href=&quot;https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context/&quot;&gt;A Survey of Long-Term Context in Transformers. Machine Learning Musings&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Mehta, S., Lu, X., Weaver, D., Elmore, J. G., Hajishirzi, H., &amp;amp; Shapiro, L. (2020) &lt;a href=&quot;http://arxiv.org/abs/2007.13007&quot;&gt;HATNet: An End-to-End Holistic Attention Network for Diagnosis of Breast Biopsy Images&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Mnih, V., Heess, N., Graves, A., &amp;amp; kavukcuoglu,  koray. (2014) &lt;a href=&quot;https://proceedings.neurips.cc/paper/2014/hash/09c6c3783b4a70054da74f2538ed47c6-Abstract.html&quot;&gt;Recurrent Models of Visual Attention. Advances in Neural Information Processing Systems, 27&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Olah, C. (2015, August 27) &lt;a href=&quot;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&quot;&gt;Understanding LSTM Networks. Colah’s Blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Olah, C., &amp;amp; Carter, S. (2016) &lt;a href=&quot;https://doi.org/10.23915/distill.00001&quot;&gt;Attention and Augmented Recurrent Neural Networks. Distill, 1(9), e1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, Ł., Shazeer, N., Ku, A., &amp;amp; Tran, D. (2018) &lt;a href=&quot;https://arxiv.org/abs/1802.05751v3&quot;&gt;Image Transformer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Pascanu, R., Mikolov, T., &amp;amp; Bengio, Y. (2013) &lt;a href=&quot;http://arxiv.org/abs/1211.5063&quot;&gt;On the difficulty of training Recurrent Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., &amp;amp; Sutskever, I. (2019) &lt;a href=&quot;https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe&quot;&gt;Language Models are Unsupervised Multitask Learners&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Reyes, M., Meier, R., Pereira, S., Silva, C. A., Dahlweid, F.-M., Tengg-Kobligk, H. von, Summers, R. M., &amp;amp; Wiest, R. (2020) &lt;a href=&quot;https://doi.org/10.1148/ryai.2020190043&quot;&gt;On the Interpretability of Artificial Intelligence in Radiology: Challenges and Opportunities. Radiology: Artificial Intelligence, 2(3), e190043&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Rubiera, C. O. (2021) &lt;a href=&quot;https://www.blopig.com/blog/2021/07/alphafold-2-is-here-whats-behind-the-structure-prediction-miracle/&quot;&gt;AlphaFold 2 is here: What’s behind the structure prediction miracle - Oxford Protein Informatics Group. Oxford Protein Informatics Group&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Rumelhart, D. E., Hinton, G., &amp;amp; Williams, R. (1987) &lt;a href=&quot;https://ieeexplore.ieee.org/document/6302929&quot;&gt;Learning Internal Representations by Error Propagation. In Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations (pp. 318–362). MIT Press&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., &amp;amp; Batra, D. (2020) &lt;a href=&quot;https://doi.org/10.1007/s11263-019-01228-7&quot;&gt;Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization. International Journal of Computer Vision, 128(2), 336–359&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Sutskever, I., Vinyals, O., &amp;amp; Le, Q. V. (2014) &lt;a href=&quot;http://arxiv.org/abs/1409.3215&quot;&gt;Sequence to Sequence Learning with Neural Networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Tay, Y., Dehghani, M., Bahri, D., &amp;amp; Metzler, D. (2020) &lt;a href=&quot;http://arxiv.org/abs/2009.06732&quot;&gt;Efficient Transformers: A Survey&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp;amp; Polosukhin, I. (2017) &lt;a href=&quot;http://arxiv.org/abs/1706.03762&quot;&gt;Attention Is All You Need&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Wikipedia. (2021) &lt;a href=&quot;https://en.wikipedia.org/w/index.php?title=Gene_expression&amp;amp;oldid=1051856939&quot;&gt;Gene expression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhutdinov, R., Zemel, R., &amp;amp; Bengio, Y. (2016) &lt;a href=&quot;http://arxiv.org/abs/1502.03044&quot;&gt;Show, Attend and Tell: Neural Image Caption Generation with Visual Attention&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., &amp;amp; Ahmed, A. (2021) &lt;a href=&quot;http://arxiv.org/abs/2007.14062&quot;&gt;Big Bird: Transformers for Longer Sequences&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Christian Garbin</name></author><category term="machine-learning" /><category term="computer-vision" /><category term="transformers" /><summary type="html">The evolution of transformers, their application in natural language processing (NLP), their surprising effectiveness in computer vision, ending with applications in healthcare.</summary></entry><entry><title type="html">Machine learning interpretability with feature attribution</title><link href="https://cgarbin.github.io/machine-learning-interpretability-feature-attribution/" rel="alternate" type="text/html" title="Machine learning interpretability with feature attribution" /><published>2021-04-26T00:00:00-04:00</published><updated>2022-12-20T00:00:00-05:00</updated><id>https://cgarbin.github.io/machine-learning-interpretability-feature-attribution</id><content type="html" xml:base="https://cgarbin.github.io/machine-learning-interpretability-feature-attribution/">&lt;p&gt;There are many discussions in the machine learning (ML) community about model interpretability and explainability. The discussions take place in several contexts, ranging from using interpretability and explainability techniques to increase the robustness of a model, all the way to increasing end-user trust in a model.&lt;/p&gt;

&lt;p&gt;This article reviews &lt;em&gt;feature attribution&lt;/em&gt;, a technique to interpret model predictions. First, it reviews commonly-used feature attribution methods, then demonstrates feature attribution with SHAP, one of these methods.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Feature attribution methods “&lt;a href=&quot;https://cloud.google.com/ai-platform/prediction/docs/ai-explanations/overview&quot;&gt;indicate how much each feature in your model contributed to the predictions for each given instance.&lt;/a&gt;” They work with tabular data, text, and images. The following pictures show an example for each case.&lt;/p&gt;

&lt;p&gt;An example of feature attribution for text (from &lt;a href=&quot;https://www.mdpi.com/1099-4300/23/1/18#&quot;&gt;Explainable AI: A Review of Machine Learning Interpretability Methods&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-26/example-feature-attribution-text.png&quot; alt=&quot;Feature attribution - text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;An example of feature attribution for tabular data (from &lt;a href=&quot;https://shap.readthedocs.io/en/latest/example_notebooks/tabular_examples/tree_based_models/Catboost%20tutorial.html&quot;&gt;SHAP tutorial - official documentation&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-26/example-feature-attribution-tabular.png&quot; alt=&quot;Feature attribution - tabular&quot; /&gt;&lt;/p&gt;

&lt;p&gt;An example of feature attribution for a model that identifies a cat in a picture (from &lt;a href=&quot;https://github.com/marcotcr/lime&quot;&gt;LIME’s GitHub&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-26/example-feature-attribution-image.png&quot; alt=&quot;Feature attribution for image identification&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-feature-attributions-are-used-for&quot;&gt;What feature attributions are used for&lt;/h2&gt;

&lt;p&gt;The prominent use cases for feature attribution are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Debug models&lt;/em&gt;: verify that models make predictions for the right reasons. For example, in the first picture below, a model predicts diseases in X-ray images based on the metal tags the X-ray technicians place on patients, not the actual disease marks (an example of &lt;a href=&quot;https://arxiv.org/abs/1907.02893&quot;&gt;spurious correlation&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Audit models&lt;/em&gt;: verify that models are not looking at attributes that encode bias (gender, race, among others) when making decisions. For example, in the second picture below, the middle column shows a gender-biased model that predicts professions by looking at the face in the image. The rightmost column shows where a debiased model looks to make predictions.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Optimize models&lt;/em&gt;: simplify correlated features and remove features that do not contribute to predictions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The figure below (&lt;a href=&quot;https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1002683&quot;&gt;source&lt;/a&gt;) is an example of feature attribution to debug a model (verify what the model uses to predict diseases). In this case, the model is looking at the wrong place to make predictions (using the X-ray markers instead of the pathology).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-26/use-debug-model.png&quot; alt=&quot;Using interpretability to debug models&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The figure below (&lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;&gt;source&lt;/a&gt;) is an example of feature attribution to audit a model. The middle column shows how the model predicts all women as “nurse”, never as “doctor” – an example of gender bias. The rightmost column shows a corrected model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-26/use-audit-model.png&quot; alt=&quot;Using interpretability to audit models&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;where-feature-attribution-is-in-relation-to-other-interpretability-methods&quot;&gt;Where feature attribution is in relation to other interpretability methods&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.05100&quot;&gt;Explainability fact sheets&lt;/a&gt; defines the following explanation families (borrowed from &lt;a href=&quot;https://dl.acm.org/doi/10.1145/169891.169951&quot;&gt;Explanation facilities and interactive systems&lt;/a&gt;):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Association between antecedent and consequent&lt;/em&gt;: “model internals such as its parameters, feature(s)-prediction relations such as explanations based on feature attribution or importance and item(s)-prediction relations, such as influential training instances”.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Contrast and differences&lt;/em&gt;: “prototypes and criticisms (similarities and dissimilarities) and class-contrastive counterfactual statements”.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Causal mechanism&lt;/em&gt;: “a full causal model”.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Feature attribution is part of the first family, the association between antecedent and consequent.&lt;/p&gt;

&lt;p&gt;Using the framework in the &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/taxonomy-of-interpretability-methods.html&quot;&gt;taxonomy of interpretable models&lt;/a&gt;, we can further narrow down feature attribution methods as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Post-hoc&lt;/em&gt;: They are usually used after the model is trained and usually with black-box models. Therefore, we are interpreting the results of the model, not the model itself (c)reating interpretable models is yet &lt;a href=&quot;https://arxiv.org/abs/1811.10154&quot;&gt;another area of research&lt;/a&gt;). The typical application for feature attribution is to interpret the predictions of black-box models, such as deep neural networks (DNNs) and random forests. These models are too complex to be directly interpreted. Thus we are left with interpreting the model’s results, not the model itself.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Result of the interpretation method&lt;/em&gt;: They result in feature summary statistics (and visualization - most summary statistics can be visualized in one way or another).&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Model-agnostic or model-specific&lt;/em&gt;: Shapley-value-based feature attribution methods can be used with different model architectures - they are model agnostic. Gradient-based feature attribution methods are based on gradients; therefore, they can be used only with models trained with gradient descent (neural networks, logistic regression, support vector machines, for example) - they are model specific.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Local&lt;/em&gt;: They explain an individual prediction of the model, not the entire model (that would be “global” interpretability).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Putting it all together, feature attribution methods are post-hoc, local interpretation methods. They can be model-agnostic (e.g., SHAP) or model-specific (e.g., Grad-CAM).&lt;/p&gt;

&lt;h2 id=&quot;limitations-and-traps-of-feature-attribution&quot;&gt;Limitations and traps of feature attribution&lt;/h2&gt;

&lt;h3 id=&quot;feature-attributions-are-approximations&quot;&gt;Feature attributions are approximations&lt;/h3&gt;

&lt;p&gt;In their typical application, explanations have a fundamental limitation when applied to black-box models: they are approximations of how the model behaves.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“[Explanations] cannot have perfect fidelity with respect to the original model. If the explanation was completely faithful to what the original model computes, the explanation would equal the original model, and one would not need the original model in the first place, only the explanation.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;small&quot;&gt;&lt;cite&gt;Cynthia Rudin&lt;/cite&gt; — &lt;a href=&quot;https://arxiv.org/abs/1811.10154&quot;&gt;Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;More succinctly:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;“Explanations must be wrong.”&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;small&quot;&gt;&lt;cite&gt;Cynthia Rudin&lt;/cite&gt; — &lt;a href=&quot;https://arxiv.org/abs/1811.10154&quot;&gt;Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As we are going through the exploration of the feature attributions, we must keep in my mind that we are analyzing two items at the same time:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What the model predicted.&lt;/li&gt;
  &lt;li&gt;How feature attribution &lt;em&gt;approximates&lt;/em&gt; what the model considers to make the prediction.&lt;/li&gt;
&lt;/ol&gt;

&lt;p class=&quot;notice--warning&quot;&gt;Therefore, &lt;strong&gt;never mistake the explanation for the actual behavior of the model&lt;/strong&gt;. This is a critical conceptual limitation to keep in mind.&lt;/p&gt;

&lt;p&gt;Because the explanations are approximations, they may disagree with each other. For example, in the figure below, LIME (left) and SHAP (right) disagree not only in the magnitude of features’ contributions but also in the direction (sign). This disagreement is more common than we may think. Refer to the excellent paper &lt;em&gt;&lt;a href=&quot;https://arxiv.org/abs/2202.01602&quot;&gt;The Disagreement Problem in Explainable Machine Learning: A Practitioner’s Perspective&lt;/a&gt;&lt;/em&gt; for more details and how practitioners deal with this issue (the figure comes from the paper).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-26/disagreement.png&quot; alt=&quot;Explanation methods may disagree&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;feature-attribution-may-not-make-sense&quot;&gt;Feature attribution may not make sense&lt;/h3&gt;

&lt;p&gt;Feature attributions do not have any understanding of the model they are explaining. They simply explain what the model predicts, &lt;a href=&quot;https://arxiv.org/abs/1811.10154&quot;&gt;not caring if the prediction is right or wrong&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-26/explaining-wrong-prediction.png&quot; alt=&quot;Explaining wrong predictions&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;notice--warning&quot;&gt;Therefore, &lt;strong&gt;never confuse “explaining” with “understanding”&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;feature-attributions-are-sensitive-to-the-baseline&quot;&gt;Feature attributions are sensitive to the baseline&lt;/h3&gt;

&lt;p&gt;Another conceptual limitation is the choice of a baseline. The attributions are not absolute values. They are the contributions compared to a baseline. To better understand why baselines are important, see how Shapley values are calculated in the &lt;a href=&quot;#shapley-values&quot;&gt;Shapley values section&lt;/a&gt;, then the section on baselines right after it.&lt;/p&gt;

&lt;h3 id=&quot;feature-attributions-are-slow-to-calculate&quot;&gt;Feature attributions are slow to calculate&lt;/h3&gt;

&lt;p&gt;Moving on to practical limitations, an important one is performance. Calculating feature attributions for large images is time-consuming.&lt;/p&gt;

&lt;p&gt;When used to help explain the predictions of a model to end-users, consider that it may make the user interface look unresponsive. You may have to compute the attributions offline or, at a minimum, indicate to the user that there is a task in progress and how long it will take.&lt;/p&gt;

&lt;h3 id=&quot;user-interactions-are-complex&quot;&gt;User interactions are complex&lt;/h3&gt;

&lt;p&gt;The attributions we get from the feature attributions algorithms are just numbers. To make sense of them, we need to apply visualization techniques.&lt;/p&gt;

&lt;p&gt;For example, simply overlaying the raw attribution values on an image may leave out important pixels that contributed to the prediction, as illustrated in figure 2 of &lt;a href=&quot;http://ceur-ws.org/Vol-2327/IUI19WS-ExSS2019-16.pdf&quot;&gt;this paper&lt;/a&gt;. Compare the number of pixels highlighted in the top-right picture with the one below it, adjusted to show more contributing pixels.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-26/user-interaction-example.png&quot; alt=&quot;Example of user interaction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Showing all information at once to the users may also induce them to make more mistakes. For example, when showing the feature attributions overlaid to a medical image, &lt;a href=&quot;https://www.aaojournal.org/article/S0161-6420(18)31575-6/fulltext&quot;&gt;this paper&lt;/a&gt; found out that it increased overdiagnosing of a medical condition. It points to the fact that just because we can explain something, we shouldn’t necessarily put that explanation in front of users without considering how it will change their behavior.&lt;/p&gt;

&lt;h2 id=&quot;well-known-feature-attribution-methods&quot;&gt;Well-known feature attribution methods&lt;/h2&gt;

&lt;p&gt;The following table was compiled with the article &lt;a href=&quot;https://thegradient.pub/a-visual-history-of-interpretation-for-image-recognition/&quot;&gt;A Visual History of Interpretation for Image Recognition&lt;/a&gt; and the paper &lt;a href=&quot;https://www.mdpi.com/1099-4300/23/1/18&quot;&gt;Explainable AI: A Review of Machine Learning Interpretability Methods&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Each row has an explanation method, when it was introduced, a link to the paper that introduced it, and an example of how the method attributes features. The entries are in chronological order.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Method and introductory paper&lt;/th&gt;
      &lt;th&gt;Feature attribution example (from the paper)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;CAM (class activation maps)&lt;br /&gt;2015-12&lt;br /&gt; &lt;a href=&quot;https://arxiv.org/abs/1512.04150&quot;&gt;Learning Deep Features for Discriminative Localization&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-04-26/method-example-cam.png&quot; alt=&quot;CAM example&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;LIME (local interpretable model-agnostic explanations)&lt;br /&gt;2016-08&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.04938&quot;&gt;“Why Should I Trust You?”: Explaining the Predictions of Any Classifier&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-04-26/method-example-lime.png&quot; alt=&quot;LIME example&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Grad-CAM&lt;br /&gt;2016-10&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/abs/1610.02391&quot;&gt;Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-04-26/method-example-grad-cam.png&quot; alt=&quot;Grad-CAM example&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Integrated gradients&lt;br /&gt;2017-03&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/abs/1703.01365&quot;&gt;Axiomatic Attribution for Deep Networks&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-04-26/method-example-integrated-gradients.png&quot; alt=&quot;integrated gradients example&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;DeepLIFT (Deep Learning Important FeaTures)&lt;br /&gt;2017-04&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.02685&quot;&gt;Learning Important Features Through Propagating Activation Differences&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-04-26/method-example-deeplift.png&quot; alt=&quot;DeepLIFT example&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SHAP (SHapley Additive exPlanations)&lt;br /&gt;2017-05&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/abs/1705.07874&quot;&gt;A Unified Approach to Interpreting Model Predictions&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-04-26/method-example-shap.png&quot; alt=&quot;SHAP example&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;SmoothGrad&lt;br /&gt;2017-06&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03825&quot;&gt;SmoothGrad: removing noise by adding noise&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-04-26/method-example-smoothgrad.png&quot; alt=&quot;SHAP example&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Anchors&lt;br /&gt;2018&lt;br /&gt;&lt;a href=&quot;https://homes.cs.washington.edu/~marcotcr/aaai18.pdf&quot;&gt;Anchors: High Precision Model-Agnostic Explanations&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-04-26/method-example-anchors.png&quot; alt=&quot;Anchors example&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;CEM (contrastive explanations method)&lt;br /&gt;2018-02&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.07623&quot;&gt;Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-04-26/method-example-cem.png&quot; alt=&quot;CEM example&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;This looks like that&lt;br /&gt;2018-06&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/abs/1806.10574&quot;&gt;This Looks Like That: Deep Learning for Interpretable Image Recognition&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-04-26/method-example-this-looks-like-that.png&quot; alt=&quot;This looks like that example&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;XRAI&lt;br /&gt;2019-06&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.02825&quot;&gt;XRAI: Better Attributions Through Regions&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-04-26/method-example-xrai.png&quot; alt=&quot;XRAI example&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Contrastive Explanations&lt;br /&gt;2021-09&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/abs/2103.01378&quot;&gt;Contrastive Explanations for Model Interpretability&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-04-26/method-example-contrastive-explanations.png&quot; alt=&quot;Contrastive explanations example&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;a-feature-attribution-example-with-shap&quot;&gt;A feature attribution example with SHAP&lt;/h2&gt;

&lt;p&gt;SHAP (SHapley Additive exPlanations) was introduced in the paper &lt;a href=&quot;https://arxiv.org/abs/1705.07874&quot;&gt;A Unified Approach to Interpreting Model Predictions&lt;/a&gt;. As the title indicates, SHAP &lt;a href=&quot;https://github.com/slundberg/shap#methods-unified-by-shap&quot;&gt;unifies&lt;/a&gt; LIME, Shapley sampling values, DeepLIFT, QII, layer-wise relevance propagation, Shapley regression values, and tree interpreter.&lt;/p&gt;

&lt;p&gt;Because of SHAP’s claim to unify several methods, in this section we review how it works. It starts with an example of SHAP for image classification, then explains the theory behind it. For a more detailed review of SHAP, including code, please see &lt;a href=&quot;/shap-experiments-image-classification/&quot;&gt;this article&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;example-with-mnist&quot;&gt;Example with MNIST&lt;/h3&gt;

&lt;p&gt;The code for the examples described in this section is available on &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/shap-experiments-image-classification&quot;&gt;this GitHub repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The following figure shows the SHAP feature attributions for a convolutional neural network that classifies digits from the MNIST dataset.&lt;/p&gt;

&lt;p&gt;The leftmost digit is the sample from the MNIST dataset. The text at the top shows the actual label from the dataset (8) and the label the network predicted (also 8, thus a correct prediction). The next ten digits are the SHAP feature attributions for each class (the digits zero to nine, from left to right). At the top of each class we see the probability assigned by the network. In this case, the network gave the probability 99.54% to the digit 8, so it’s correct and very confident about the prediction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-26/shap-example-mnist.png&quot; alt=&quot;SHAP example with MNIST&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SHAP uses colors to explain attributions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Red pixels&lt;/em&gt; increases the probability of a class being predicted&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Blue pixels&lt;/em&gt; decrease the probability of a class being predicted&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can see that the contours of the digit 8 are assigned high probability. We can also see that the empty space inside the top loop is relevant to detecting a digit 8. The empty spaces to the left and right of the middle, where the bottom and top half of the digit meet are also important. In other words, it’s not only what is present that is important to decide what digit an image is, but also what is absent.&lt;/p&gt;

&lt;p&gt;Looking at digits 2 and 3, we can see in blue the reasons why the network assigned lower probabilities to them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-26/shap-colors.png&quot; alt=&quot;SHAP color coding example&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;shapley-values&quot;&gt;Shapley values&lt;/h3&gt;

&lt;p&gt;SHAP uses an approximation of Shapley value for feature attribution. The Shapley value determines the contribution of individuals in interactions that involve multiple participants.&lt;/p&gt;

&lt;p&gt;For example (based on &lt;a href=&quot;https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf&quot;&gt;this article&lt;/a&gt;), a company has three employees, Anne, Bob, and Charlie. The company has ended a month with a profit of 100 (the monetary unit is not essential). The company wants to distribute the profit to the employees according to their contribution.&lt;/p&gt;

&lt;p&gt;We have so far two pieces of information, the profit when the company had no employee (zero) and the profit with all three employees on board.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Employees&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Profit&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;em&gt;None&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Anne, Bob, Charlie&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;100&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Going through historical records, the company determined the profit when different combinations of employees were working in the past months. They are added to the table below, between the two lines of the previous table.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt; &lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Employees&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Profit&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;em&gt;None&lt;/em&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Anne&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Bob&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Charlie&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Anne, Bob&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;60&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Bob, Charlie&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;70&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Anne, Charlie&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Anne, Bob, Charlie&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;100&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;At first glance, it looks like Bob contributes 50 to the profit: in line 2 we see that Anne contributes 10 to the profit and in line 5 the profit of Anne and Bob together is 60. The conclusion would be that Bob contributed 50. However, when we look at line 4 (only Charlie) and line 6 (Bob and Charlie), we now conclude that Bob contributes 40 to the profit, contradicting the first conclusion.&lt;/p&gt;

&lt;p&gt;Which one is correct? Both. We are interested in each employee’s contribution when they are working together. &lt;em&gt;This is a collaborative game&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To understand the individual contributions, we start by analyzing all possible paths from “no employee” to “all three employees”.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Path&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Combination to get to all employees&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Anne → Anne, Bob → Anne, Bob, Charlie&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Anne → Anne, Charlie → Anne, Bob, Charlie&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Bob → Anne, Bob → Anne, Bob, Charlie&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Bob → Bob, Charlie → Anne, Bob, Charlie&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Charlie → Anne, Charlie → Anne, Bob, Charlie&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Charlie → Bob, Charlie → Anne, Bob, Charlie&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We then calculate each employee’s contribution in that path (this part is important). For example, in the first path, Anne contributes 10 (line 1 in the previous table), Bob contributes 50 (line 5, minus Anne’s contribution of 10), and Charlie contributes 40 (line 8 in the previous table, minus line 5). The total contribution must add to the total profit (this part is also important): Anne = 10 + Bob = 50 + Charlie = 40 → 100.&lt;/p&gt;

&lt;p&gt;Repeating the process above, we calculate each employee’s contribution for each path. Finally, we average the contributions — &lt;strong&gt;this is the Shapley value for each employee&lt;/strong&gt; (last line in the table).&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Path&lt;/th&gt;
      &lt;th style=&quot;text-align: left&quot;&gt;Combination to get to all employees&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Anne&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Boob&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Charlie&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Anne → Anne, Bob → Anne, Bob, Charlie&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;50&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Anne → Anne, Charlie → Anne, Bob, Charlie&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;80&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Bob → Anne, Bob → Anne, Bob, Charlie&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;40&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;20&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;40&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Bob → Bob, Charlie → Anne, Bob, Charlie&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;30&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;20&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;50&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Charlie → Anne, Charlie → Anne, Bob, Charlie&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;30&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;40&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;Charlie → Bob, Charlie → Anne, Bob, Charlie&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;60&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt; &lt;/td&gt;
      &lt;td style=&quot;text-align: left&quot;&gt;&lt;strong&gt;Average (Shapley value)&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;strong&gt;30&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;strong&gt;25&lt;/strong&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;&lt;strong&gt;45&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In this example we managed to calculate each individual’s contribution for all possible paths in a reasonable time. In machine learning, the “individuals” are the features in the dataset. There may be thousands or even millions of features in a dataset. For example, in image classification, each pixel in the image is a feature.&lt;/p&gt;

&lt;p&gt;SHAP uses a similar method to explain the contribution of features to a model’s prediction. However, calculating the contribution of each feature is not feasible in some cases (e.g. images and their millions of pixels). The combination of paths to try is exponential (factorial, to be precise). SHAP makes simplifications to calculate the features’ contributions. It is crucial to remember that &lt;strong&gt;SHAP is an approximation, not the actual contribution value&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;the-importance-of-the-baseline&quot;&gt;The importance of the baseline&lt;/h3&gt;

&lt;p&gt;In the example above, we asked “what is each employee’s contribution to the profit?”. Our baseline was the company with zero employees and no profit.&lt;/p&gt;

&lt;p&gt;We could have asked a different question: “what is the contribution of Bob and Charlie, given that Anne is already an employee?”. In this case, our baseline is 10, the profit that Anne adds to the company by herself. Only paths 1 and 2 would apply, with the corresponding changes to the average contribution.&lt;/p&gt;

&lt;p&gt;SHAP (and other feature attribution methods) calculate the feature contribution compared to a baseline. For example, in feature attribution for image classification, the baseline is an image or a set of images.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The choice of the baseline affects the calculations&lt;/em&gt;. &lt;a href=&quot;https://distill.pub/2020/attribution-baselines/&quot;&gt;Visualizing the Impact of Feature Attribution Baselines&lt;/a&gt; discussed the problem and its effect on feature attribution.&lt;/p&gt;

&lt;h2 id=&quot;appendix---interpretability-vs-explainability&quot;&gt;Appendix - interpretability vs. explainability&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.manning.com/books/interpretable-ai&quot;&gt;Ajay Thampi’s Interpretable AI&lt;/a&gt; book distinguishes between interpretability and explainability this way:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Interpretability&lt;/em&gt;: “It is the degree to which we can consistently estimate what a model will predict given an input, understand how the model came up with the prediction, understand how the prediction changes with changes in the input or algorithmic parameters and finally understand when the model has made a mistake. Interpretability is mostly discernible by experts who are either building, deploying or using the AI system and these techniques are building blocks that will help you get to explainability.”&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Explainability&lt;/em&gt;: “[G]oes beyond interpretability in that it helps us understand in a human-readable form how and why a model came up with a prediction. It explains the internal mechanics of the system in human terms with the intent to reach a much wider audience. Explainability requires interpretability as building blocks and also looks to other fields and areas such as Human-Computer Interaction (HCI), law and ethics.”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Other sources treat interpretability and explainability as equivalent terms (for example, &lt;a href=&quot;https://arxiv.org/abs/1706.07269&quot;&gt;Miller’s work&lt;/a&gt; and &lt;a href=&quot;https://christophm.github.io/interpretable-ml-book/interpretability.html&quot;&gt;Molan’s online book on the topic&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This article uses “interpretability” as defined in Ajay Thampi’s book. We distinguish between interpretability and explainability to not involve aspects of displaying the interpretation of a model’s prediction to end-users. This would add to the discussion other topics such as user interface and user interaction. While important for the overall discussion of ML interpretability and explainability, these topics are not relevant to the scope of this work. However, we preserve the original term when quoting a source. If the source chose “explainability”, we quote it so.&lt;/p&gt;

&lt;p&gt;Therefore, when we discuss “interpretability” here, we mean the interpretation that is shown to a machine learning practitioner, someone familiar with model training and evaluation. We discuss interpretability in a more technical format with this definition in place, assuming that the consumer of the interpretability results has enough technical background to understand it.&lt;/p&gt;</content><author><name>Christian Garbin</name></author><category term="machine-learning" /><category term="explainability" /><category term="interpretability" /><category term="shap" /><summary type="html">A review of _feature attribution_, a technique to interpret model predictions. First, it reviews commonly-used feature attribution methods, then demonstrates feature attribution with SHAP, one of these methods.</summary></entry><entry><title type="html">An overview of deep learning for image processing</title><link href="https://cgarbin.github.io/deep-learning-for-image-processing-overview/" rel="alternate" type="text/html" title="An overview of deep learning for image processing" /><published>2021-04-26T00:00:00-04:00</published><updated>2022-02-04T00:00:00-05:00</updated><id>https://cgarbin.github.io/deep-learning-for-image-processing-overview</id><content type="html" xml:base="https://cgarbin.github.io/deep-learning-for-image-processing-overview/">&lt;p&gt;Deep learning revolutionized image processing. It made previous techniques, based on manual feature extraction, obsolete. This article reviews the progress of deep learning, with ever-growing networks and the new developments in the field.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Deep learning is a sub-area of machine learning, which in turn is a sub-area of artificial intelligence (&lt;a href=&quot;https://commons.wikimedia.org/wiki/File:AI-ML-DL.svg&quot;&gt;picture source&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-02-28/AI-ML-DL.png&quot; alt=&quot;Deep learning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The best way I found to explain deep learning is in contrast to traditional methods. Yann LeCun, one of the founders of deep learning, gave an &lt;a href=&quot;https://www.youtube.com/watch?v=Qk4SqF9FT-M&quot;&gt;informative talk&lt;/a&gt; on the evolution of learning techniques, starting with the traditional ones and ending with deep learning. He focuses on image recognition in that talk.&lt;/p&gt;

&lt;p&gt;It is a worthwhile investment of one hour of our time to listen to someone who was not only present but actively driving the evolution of deep learning. The two pictures immediately below are from his speech.&lt;/p&gt;

&lt;h2 id=&quot;traditional-image-recognition-vs-deep-learning&quot;&gt;Traditional image recognition vs. deep learning&lt;/h2&gt;

&lt;p&gt;In traditional image recognition, we use hand-crafted rules to extract features from an image (&lt;a href=&quot;https://youtu.be/Qk4SqF9FT-M?t=305&quot;&gt;source&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-02-28/image-processing-traditional.png&quot; alt=&quot;Traditional image processing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In contrast, deep learning image recognition is done with trainable, multi-layer neural networks. Instead of hand-crafting the rules, we feed labeled images to the network. The neural network, through the training process, extracts the features needed to identify the images (&lt;a href=&quot;https://youtu.be/Qk4SqF9FT-M?t=435&quot;&gt;source&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-02-28/image-processing-deep-learning.png&quot; alt=&quot;Deep learning image processing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;“Deep” comes from the fact that neural networks (in this application) use several layers. For example, LeNet-5, named after Yann LeCunn (of the presentation above) and shown in the (historic) picture below (&lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf&quot;&gt;source&lt;/a&gt;), has seven layers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-02-28/lenet-5.png&quot; alt=&quot;Deep learning network example&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;what-deep-learning-networks-learn&quot;&gt;What deep learning networks “learn”&lt;/h2&gt;

&lt;p&gt;Each layer “learns” (“extracts” is a better technical term) different aspects (“features” in the pictures above) of the images. Lower layers extract basic features (such as edges), and higher layers extract more complex concepts (that frankly, &lt;a href=&quot;https://distill.pub/2017/feature-visualization/&quot;&gt;we don’t quite know how to explain yet&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The picture below (&lt;a href=&quot;https://distill.pub/2017/feature-visualization/&quot;&gt;source&lt;/a&gt;) shows the features that each layer of a deep learning network extracts. On the left, we have the first layers of the network. They extract basic features, such as edges. As we move to the right, we see the upper layers of the network and the features they extract.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-02-28/layer-visualization.png&quot; alt=&quot;Visualization of features in layers of a network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Unlike traditional image processing, a deep learning network is not manually configured to extract these features. They learn it through the &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/training-neural-networks/video-lecture&quot;&gt;training process&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-evolution-of-deep-learning&quot;&gt;The evolution of deep learning&lt;/h2&gt;

&lt;p&gt;Deep learning for image processing entered the mainstream in the late 1990s when &lt;a href=&quot;https://cs231n.github.io/convolutional-networks/&quot;&gt;convolutional neural networks&lt;/a&gt; were applied to image processing. After stalling a bit in the early 2000s, deep learning took off in the early 2010s. In a short span of a few years, bigger and bigger network architectures were developed. Over time, what “deep” meant was stretched even further.&lt;/p&gt;

&lt;p&gt;The table below shows the evolution of deep learning network architectures.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;When/What&lt;/th&gt;
      &lt;th&gt;Notable features&lt;/th&gt;
      &lt;th&gt;Canonical depiction&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1990s&lt;br /&gt;&lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf&quot;&gt;LeNet&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Trainable network for image recognition.&lt;br /&gt;- Gradient-based learning&lt;br /&gt;- Convolutional neural network&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-02-28/lenet-5.png&quot; alt=&quot;LeNet&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2012&lt;br /&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&quot;&gt;AlexNet&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;One network outperformed, by a large margin, model ensembling (best in class at the time) in &lt;a href=&quot;https://www.image-net.org/&quot;&gt;ImageNet&lt;/a&gt;.&lt;br /&gt;- Deep convolutional neural network&lt;br /&gt;- Overcame overfitting with data augmentation and dropout&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-02-28/alexnet.png&quot; alt=&quot;AlexNet&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2014&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.4842.pdf&quot;&gt;Inception&lt;/a&gt;&lt;br /&gt;(GoogLeNet)&lt;/td&gt;
      &lt;td&gt;Very deep network (over 20 layers), composed of building blocks, resulting in a “network in a network” (inception).&lt;/td&gt;
      &lt;td&gt;Partial depiction&lt;br /&gt;&lt;img src=&quot;/images/2021-02-28/inception.png&quot; alt=&quot;Inception&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2014&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/pdf/1409.1556.pdf&quot;&gt;VGGNet&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Stacks of small convolution filters (as opposed to one large filter) to reduce the number of parameters in the network.&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-02-28/vggnet.png&quot; alt=&quot;AlexNet&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2015&lt;br /&gt;&lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;ResNet&lt;/a&gt;&lt;/td&gt;
      &lt;td&gt;Introduced skip connections (residual learning) to train very deep networks (152 layers). At the same time, the network is compact (few parameters for its size).&lt;/td&gt;
      &lt;td&gt;Partial depiction&lt;br /&gt;&lt;img src=&quot;/images/2021-02-28/resnet.png&quot; alt=&quot;Inception&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Network architectures continue to evolve today. So many architectures have been put into practice that we now need a &lt;a href=&quot;https://arxiv.org/abs/1901.06032&quot;&gt;taxonomy to categorize them&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-02-28/taxonomy.png&quot; alt=&quot;CNN taxonomy&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;recent-trends&quot;&gt;Recent trends&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Efficiently scaling CNNs&lt;/em&gt;: There are different ways to scale CNN-based networks. The &lt;a href=&quot;https://arxiv.org/abs/1905.11946&quot;&gt;EfficientNet&lt;/a&gt; family of networks shows that we don’t always need large CNN networks to get good results.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Back to basics&lt;/em&gt;: The &lt;a href=&quot;https://arxiv.org/abs/2105.01601&quot;&gt;MLP-Mixer&lt;/a&gt; network does away with CNN layers altogether. It uses only simpler multi-layer perceptron (MLP) layers, resulting in networks with faster throughput, predicting more images per second than other network architectures.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Transformers&lt;/em&gt;: Transformer-based networks, after their success with natural language processing (NLP), &lt;a href=&quot;/transformers-in-computer-vision/&quot;&gt;are being applied to image processing&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Learning concepts&lt;/em&gt;: by training with images and their textual descriptions (multimodal learning), OpenAI created &lt;a href=&quot;https://openai.com/blog/clip/&quot;&gt;CLIP&lt;/a&gt;, a network that seems to have learned the concepts of images. Traditional image classification relied on extracting features from the images. They work well on images with the same characteristics but fail when they are different. For example, they identify the picture of a banana but not the sketch of a banana. On the other hand, CLIP seems to have learned the concept of the images. It identifies pictures and sketches of bananas (see the illustration in the &lt;a href=&quot;https://openai.com/blog/clip/&quot;&gt;article&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;keeping-up-with-new-developments&quot;&gt;Keeping up with new developments&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://paperswithcode.com/&quot;&gt;Papers with Code&lt;/a&gt; maintains a &lt;a href=&quot;https://paperswithcode.com/sota/image-classification-on-imagenet&quot;&gt;leaderboard of the state of the art&lt;/a&gt;, including links to the papers that describe the network used to achieve each result.&lt;/p&gt;</content><author><name>Christian Garbin</name></author><category term="machine-learning" /><category term="deep-learning" /><category term="computer-vision" /><summary type="html">Deep learning (large, multi-layered neural networks) have been successfully applied to computer vision tasks. This article reviews its origins, the evolution of network architectures, and recent developments.</summary></entry><entry><title type="html">Exploring SHAP explanations for image classification</title><link href="https://cgarbin.github.io/shap-experiments-image-classification/" rel="alternate" type="text/html" title="Exploring SHAP explanations for image classification" /><published>2021-04-25T00:00:00-04:00</published><updated>2021-04-25T00:00:00-04:00</updated><id>https://cgarbin.github.io/shap-experiments-image-classification</id><content type="html" xml:base="https://cgarbin.github.io/shap-experiments-image-classification/">&lt;p&gt;This article explores how to interpret predictions of an image classification neural network using &lt;a href=&quot;https://arxiv.org/abs/1705.07874&quot;&gt;SHAP (SHapley Additive exPlanations)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The goals of the experiments are to:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Explore how SHAP explains the predictions. This experiment uses a (fairly) accurate network to understand how SHAP attributes the predictions.&lt;/li&gt;
  &lt;li&gt;Explore how SHAP behaves with inaccurate predictions. This experiment uses a network with lower accuracy and prediction probabilities that are less robust (more spread among the classes) to understand how SHAP behaves when the predictions are not reliable (a hat tip to &lt;a href=&quot;https://arxiv.org/abs/1811.10154&quot;&gt;Dr. Rudin’s work&lt;/a&gt;).&lt;/li&gt;
&lt;/ol&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;why-use-shap-instead-of-another-method&quot;&gt;Why use SHAP instead of another method?&lt;/h2&gt;

&lt;p&gt;This project is my first opportunity to delve into model interpretability at the code level. I picked &lt;a href=&quot;https://arxiv.org/abs/1705.07874&quot;&gt;SHAP (SHapley Additive exPlanations)&lt;/a&gt; to get started because of &lt;a href=&quot;https://github.com/slundberg/shap#methods-unified-by-shap&quot;&gt;its promise to unify various methods&lt;/a&gt; (emphasis ours):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“…various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, &lt;b&gt;we present a unified framework for interpreting predictions&lt;/b&gt;, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures. … The new class unifies six existing methods, …”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;overview-of-shap-feature-attribution-for-image-classification&quot;&gt;Overview of SHAP feature attribution for image classification&lt;/h2&gt;

&lt;h3 id=&quot;how-shap-works&quot;&gt;How SHAP works&lt;/h3&gt;

&lt;p&gt;SHAP is based on &lt;a href=&quot;https://en.wikipedia.org/wiki/Shapley_value&quot;&gt;Shapley value&lt;/a&gt;, a method to calculate the contributions of each player to the outcome of a game. See &lt;a href=&quot;https://cgarbin.github.io/machine-learning-interpretability-feature-attribution/#shapley-values&quot;&gt;this article&lt;/a&gt; for a simple, illustrated example of how to calculate the Shapley value and &lt;a href=&quot;https://towardsdatascience.com/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30&quot;&gt;this article by Samuelle Mazzanti&lt;/a&gt; for a more detailed explanation.&lt;/p&gt;

&lt;p&gt;The Shapley value is calculated with all possible combinations of players. Given N players, it has to calculate outcomes for 2^N combinations of players. In the case of machine learning, the “players” are the features (e.g. pixels in an image) and the “outcome of a game” is the model’s prediction. Calculating the contribution of each feature is not feasible for large numbers of N. For example, for images, N is the number of pixels.&lt;/p&gt;

&lt;p&gt;Therefore, SHAP does not attempt to calculate the actual Shapley value. Instead, it uses sampling and approximations to calculate the SHAP value. See &lt;a href=&quot;https://arxiv.org/abs/1705.07874&quot;&gt;chapter 4 of the SHAP paper for details&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;visualizing-shap-attributions&quot;&gt;Visualizing SHAP attributions&lt;/h3&gt;

&lt;p&gt;SHAP uses colors to explain attributions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Red pixels increase the probability of a class being predicted&lt;/li&gt;
  &lt;li&gt;Blue pixels decrease the probability of a class being predicted&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following picture and text come from the &lt;a href=&quot;https://github.com/slundberg/shap#deep-learning-example-with-deepexplainer-tensorflowkeras-models&quot;&gt;SHAP README&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-25/example-from-shap-readme.png&quot; alt=&quot;SHAP example&quot; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model’s output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each explanation. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the ‘zero’ image the blank middle is important, while for the ‘four’ image the lack of a connection on top makes it a four instead of a nine.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This is an essential part of the explanation: &lt;em&gt;“Note that for the ‘zero’ image the blank middle is important, while for the ‘four’ image the lack of a connection on top makes it a four instead of a nine.”&lt;/em&gt; In other words, it’s not only what is present that is important to decide what digit an image is, but also &lt;strong&gt;&lt;em&gt;what is absent&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;experiments&quot;&gt;Experiments&lt;/h2&gt;

&lt;p&gt;This &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/shap-experiments-image-classification/blob/master/shap-experiments-image-classification.ipynb&quot;&gt;Jupyter notebook&lt;/a&gt; shows how to use SHAP’s DeepExplainer to visualize feature attribution in image classification with neural networks. See the &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/shap-experiments-image-classification/blob/master/running-the-code.md&quot;&gt;instructions to run the code&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;SHAP has multiple explainers. The notebook uses the DeepExplainer explainer because it is the one used in &lt;a href=&quot;https://shap.readthedocs.io/en/latest/image_examples.html&quot;&gt;the image classification SHAP sample code&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The code is based on the &lt;a href=&quot;https://shap.readthedocs.io/en/stable/example_notebooks/image_examples/image_classification/PyTorch%20Deep%20Explainer%20MNIST%20example.html&quot;&gt;SHAP MNIST example&lt;/a&gt;, available as a Jupyter notebook &lt;a href=&quot;https://github.com/slundberg/shap/blob/master/notebooks/image_examples/image_classification/PyTorch%20Deep%20Explainer%20MNIST%20example.ipynb&quot;&gt;on GitHub&lt;/a&gt;. This notebook uses the PyTorch sample code because at this time (April 2021), SHAP does not support TensorFlow 2.0. &lt;a href=&quot;https://github.com/slundberg/shap/issues/850&quot;&gt;This GitHub issue&lt;/a&gt; tracks the work to support TensorFlow 2.0 in SHAP.&lt;/p&gt;

&lt;p&gt;The experiments are as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Train a CNN to classify the MNIST dataset.&lt;/li&gt;
  &lt;li&gt;Show the feature attributions for a subset of the training set using SHAP DeepExplainer.&lt;/li&gt;
  &lt;li&gt;Review and annotate some of the attributions to better understand what they reveal about the model and the explanation itself.&lt;/li&gt;
  &lt;li&gt;Repeat the steps above with the CNN that is significantly less accurate.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;an-important-caveat&quot;&gt;An important caveat&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;“Explanations must be wrong.”&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;small&quot;&gt;&lt;cite&gt;Cynthia Rudin&lt;/cite&gt; — &lt;a href=&quot;https://arxiv.org/abs/1811.10154&quot;&gt;Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As we are going through the exploration of the feature attributions, we must keep in my mind that we are analyzing two items at the same time:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What the model predicted.&lt;/li&gt;
  &lt;li&gt;How the feature attribution explainer &lt;em&gt;approximates&lt;/em&gt; what the model considers to make the prediction.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The explainer &lt;em&gt;approximates&lt;/em&gt; the model and sometimes (as in this case) also uses an approximation of the input. Therefore, some of the attributions that may not make much sense may result from these approximations, not necessarily the model’s behavior.&lt;/p&gt;

&lt;p class=&quot;notice--warning&quot;&gt;Therefore, &lt;strong&gt;never mistake the explanation for the actual behavior of the model&lt;/strong&gt;. This is a critical conceptual limitation to keep in mind.&lt;/p&gt;

&lt;p&gt;See more on &lt;a href=&quot;/machine-learning-interpretability-feature-attribution/&quot;&gt;this post about feature attribution&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;some-results-from-the-experiments&quot;&gt;Some results from the experiments&lt;/h2&gt;

&lt;p&gt;This section explores some of the feature attributions resulting from the experiments (see the &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/shap-experiments-image-classification/blob/master/shap-experiments-image-classification.ipynb&quot;&gt;notebook&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Before reading further: this is my first foray into the details of feature attribution with SHAP (or any other method). Some of the items reported below are questions I need to investigate further to understand better how feature attribution in general, and SHAP in particular, work.&lt;/p&gt;

&lt;p&gt;Some candidates for research questions are noted in the explanations.&lt;/p&gt;

&lt;h3 id=&quot;accurate-network&quot;&gt;Accurate network&lt;/h3&gt;

&lt;p&gt;This section explores feature attribution using the (fairly) accurate network. This network achieves 97% overall accuracy.&lt;/p&gt;

&lt;p&gt;Each picture below shows these pieces of information:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The leftmost digit is the example from the MNIST dataset that the network predicted. The text at the top of the picture shows the actual and predicted values. The predicted value is the largest of all probabilities (without applying a threshold).&lt;/li&gt;
  &lt;li&gt;Following that digit, there are ten digits, one for each class (from left to right: zero to nine), with the feature attributions overlaid on each digit. The text at the top shows the probability that the network assigned for that class.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some of the feature attributions are easy to interpret. For example, this is the attribution for a digit “1”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-25/accurate-digit-1.png&quot; alt=&quot;SHAP attributions for digit 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see that the presence of the vertical pixels at the center of the image increases the probability of predicting a digit “1”, as we would expect. The absence of pixels around that vertical line also increases the probability.&lt;/p&gt;

&lt;p&gt;The two examples for the digit “8” below are also easy to interpret. We can see that the blank space in the top loop and the blank spaces on both sides of the middle part of the image are important to define an “8”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-25/accurate-digit-8-1.png&quot; alt=&quot;SHAP attributions for digit 8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-25/accurate-digit-8-2.png&quot; alt=&quot;SHAP attributions for digit 8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the two examples for the digit “2” below, on the other hand, the first one is easy to interpret, but the attributions for the second make less sense. While reviewing them, note that the scale for the SHAP values is different for each example. The range of values in the second example is an order of magnitude larger. It does not affect a comparative analysis but it may be important in other cases to note the scale before judging the attributions.&lt;/p&gt;

&lt;p&gt;In the first example we can see which pixels are more relevant (red) to predict the digit “2”. We can also see what pixels were used to reduce the probability of predicting the digit “7” (blue), the second-highest predicted probability.&lt;/p&gt;

&lt;p&gt;In the second picture, the more salient attributions are on the second-highest probability, the digit “7”. It’s almost as if the network “worked harder” to reject that digit than to predict the digit “2”. Although the probability of the digit “7” is higher in this second example (compared to the digit “7” in the first example), it’s still far away from the probability assigned to the digit “2”.&lt;/p&gt;

&lt;p class=&quot;notice--info&quot;&gt;&lt;strong&gt;RESEARCH QUESTION 1&lt;/strong&gt;: What causes SHAP sometimes to highlight the attributions of a class that was not assigned the highest probability?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-25/accurate-digit-2-1.png&quot; alt=&quot;SHAP attributions for digit 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-25/accurate-digit-2-2.png&quot; alt=&quot;SHAP attributions for digit 2&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;inaccurate-network&quot;&gt;Inaccurate network&lt;/h3&gt;

&lt;p&gt;This section explores feature attribution using the inaccurate network. This network achieves 87% overall accuracy. Besides the low overall accuracy, each prediction has a larger probability spread. In some cases, the difference between the largest and the second-largest probability is very small, as we will soon see.&lt;/p&gt;

&lt;p&gt;In the example for the digit “0” below, the network incorrectly predicted it as “5”. But it didn’t miss by much. The difference in probability between “5” (incorrect) and “0” (correct) is barely 1%. Also, the two probabilities add up to 54%. In other words, the two top probabilities add up to about half of the total probability. The prediction for this example is not only wrong but uncertain across several classes (labels).&lt;/p&gt;

&lt;p&gt;SHAP still does what we ask: shows the feature attributions for each class. For the three classes with the highest probability, we can see that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Digit “0”: The empty middle is the important part, as we have seen in other cases for this digit.&lt;/li&gt;
  &lt;li&gt;Digit “8”: The top and bottom parts look like the top and bottom loops of the digit “8”, resulting in the red areas we see in the attribution. The empty middle is now a detractor for this class (blue). An actual digit “8” would have something here, where the bottom and top loops meet.&lt;/li&gt;
  &lt;li&gt;Digit “5”: Left this one for last because it is the one with the highest probability (but not by much) and also the one hardest to explain. It is almost as if just a few pixels (in red) were enough to assign a probability higher than the correct digit “0”.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-25/inaccurate-digit-0.png&quot; alt=&quot;SHAP attributions for digit 0&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This example shows an important concept about explanations for black-box models: they explain what the model is predicting, but they do not attempt to explain if the predictions are correct.&lt;/p&gt;

&lt;p&gt;Hence the call to &lt;a href=&quot;https://arxiv.org/abs/1811.10154&quot;&gt;stop explaining black-box models&lt;/a&gt; (at least for some applications). But this is a story for another day…&lt;/p&gt;

&lt;h3 id=&quot;aggregate-attributions-for-accurate-vs-inaccurate-networks&quot;&gt;Aggregate attributions for accurate vs. inaccurate networks&lt;/h3&gt;

&lt;p&gt;Instead of plotting attributions one by one, as we saw in the previous examples, SHAP can also plot multiple images in the same plot. One advantage of this plot is that all images share the same SHAP scale.&lt;/p&gt;

&lt;p&gt;The plots below show all the attributions for all test digits. The accurate network is on the left and the inaccurate network is on the right.&lt;/p&gt;

&lt;p&gt;In the plot for the accurate network we can see that all samples have at least one class (digit) with favorable attributions (red). The plot is dotted with red areas. In the inaccurate network we don’t see the same pattern. The plot is mainly gray.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Accurate&lt;/th&gt;
      &lt;th&gt;Inaccurate&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-04-25/accurate-all.png&quot; alt=&quot;Accurate&quot; /&gt;&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;/images/2021-04-25/inaccurate-all.png&quot; alt=&quot;Inaccurate&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p class=&quot;notice--info&quot;&gt;&lt;strong&gt;RESEARCH QUESTION 2&lt;/strong&gt;: Given this pattern, is it possible to use the distribution of attributions across samples to determine if a network is accurate (or not)? In other words, if all we have is the feature attributions for a reasonable number of cases but don’t have the actual vs. predicted labels, could we use that to determine whether a network is accurate (or not)?&lt;/p&gt;

&lt;h2 id=&quot;limitations-of-these-experiments&quot;&gt;Limitations of these experiments&lt;/h2&gt;

&lt;p&gt;SHAP attributes features based on a baseline input. This is this line of code in the Jupyter notebook:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;expl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DeepExplainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;background_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The baseline images are extracted from the test set here:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;BACKGROUND_SIZE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;background_images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BACKGROUND_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The choice of baseline images can significantly affect the SHAP results (the results of any method that relies on baseline images, to be precise), as demonstrated in &lt;a href=&quot;https://distill.pub/2020/attribution-baselines/&quot;&gt;Visualizing the Impact of Feature Attribution Baseline&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the experiments we conducted here we used a relatively small set of images for the baseline and we didn’t attempt to get an equal distribution of the digits in that baseline (other than a simple manual check of distributions - see the notebook).&lt;/p&gt;

&lt;p class=&quot;notice--info&quot;&gt;&lt;strong&gt;RESEARCH QUESTION 3&lt;/strong&gt;: Would a larger number of baseline images, with equal distribution of digits, significantly affect the results? More generically, what is a reasonable number of baseline images to start trusting the results?&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;See instructions &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/shap-experiments-image-classification/blob/master/running-the-code.md&quot;&gt;here&lt;/a&gt; to prepare the environment and run the code.&lt;/p&gt;</content><author><name>Christian Garbin</name></author><category term="machine-learning" /><category term="computer-vision" /><category term="image-classification" /><category term="explainability" /><category term="interpretability" /><category term="shap" /><summary type="html">How to interpret predictions of an image classification neural network using SHAP.</summary></entry><entry><title type="html">Machine learning, but not understanding</title><link href="https://cgarbin.github.io/machine-learning-but-not-understanding/" rel="alternate" type="text/html" title="Machine learning, but not understanding" /><published>2021-04-10T00:00:00-04:00</published><updated>2021-04-10T00:00:00-04:00</updated><id>https://cgarbin.github.io/machine-learning-but-not-understanding</id><content type="html" xml:base="https://cgarbin.github.io/machine-learning-but-not-understanding/">&lt;p&gt;In the expression &lt;em&gt;machine learning&lt;/em&gt;, are the machines actually learning anything?&lt;/p&gt;

&lt;p&gt;In the book “Artificial Intelligence, a guide for thinking humans” Melanie Mitchell explains that&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Learning in neural networks simply consists in gradually modifying the weights on connections so that each output’s error gets as close to 0 as possible on all training examples.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;small&quot;&gt;&lt;cite&gt;Melanie Mitchell&lt;/cite&gt; — Artificial Intelligence, a guide for thinking humans&lt;/p&gt;

&lt;p&gt;Let’s explore what “learning” means for machine learning, guided by Mitchell’s book. More specifically, we will concentrate on “deep learning”, a branch of machine learning that has powered most of the recent advances in artificial intelligence.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;All quoted text in this article is from Dr. Mitchell’s book “Artificial Intelligence, a guide for thinking humans”.&lt;/p&gt;

&lt;h1 id=&quot;an-extremely-short-explanation-of-deep-learning&quot;&gt;An extremely short explanation of deep learning&lt;/h1&gt;

&lt;p&gt;Deep learning uses layers of “units”’ (also called &lt;em&gt;neurons&lt;/em&gt;, but some people, including Mitchell and I, prefer the more generic &lt;em&gt;units&lt;/em&gt; term, to not confuse with biological neurons) to extract patterns from labeled data. The internal layers are called “hidden layers”. The last layer is called the “output layer”, or the classification layer.&lt;/p&gt;

&lt;p&gt;In the following figure (from Mitchell’s book), a neural network comprised of several hidden layers (only one shown) was trained to classify handwritten digits. The output layer has ten units, one for each possible digit.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-10/neural-network.png&quot; alt=&quot;From Mitchell, Artificial Intelligence, chapter 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;How does a neural network learn? Back to Mitchell’s quote:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Learning in neural networks simply consists in gradually modifying the weights on connections so that each output’s error gets as close to 0 as possible on all training examples.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Going through the sentence pieces:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;training examples&lt;/em&gt;: The labeled examples we present to the network to train it. For example, we present a picture of a square or a triangle and its corresponding label, “square” or “triangle”.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;output’s error&lt;/em&gt;: How far the network’s prediction is from the correct label of the example picture.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;weights on connections&lt;/em&gt;: A large-precision decimal number that adjusts the output of a unit in one layer to the input of a unit in the next layer. The weights are where the “knowledge” of the neural network is encoded.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;gradually modifying&lt;/em&gt;: This is the neural network learning process. An algorithm carefully modifies the weights on the connections to get closer to the expected output. Repeating the adjustment step over time (many, many times) allows the network to learn from the training examples.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;an-important-consequence-of-this-process&quot;&gt;An important consequence of this process&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The machine learns what it observes in the data rather than what you (the human) might observe. If there are statistical associations in the training data, even if irrelevant to the task at hand, the machine will happily learn those instead of what you wanted it to learn.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Thus, neural networks are not “learning” in the sense that we would understand the term. They are not learning higher-level concepts from the samples used to train them. They are extracting patterns from the data presented to them during training (and they assume that the labels are correct). That’s all.&lt;/p&gt;

&lt;p&gt;Or, as Mitchell puts more eloquently:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The phrase “barrier of meaning” perfectly captures an idea that has permeated this book: humans, in some deep and essential way, understand the situations they encounter, whereas no AI system yet possesses such understanding. While state-of-the-art AI systems have nearly equaled (and in some cases surpassed) humans on certain narrowly defined tasks, these systems all lack a grasp of the rich meanings humans bring to bear in perception, language, and reasoning. This lack of understanding is clearly revealed by the un-humanlike errors these systems can make; by their difficulties with abstracting and transferring what they have learned; by their lack of commonsense knowledge; … The barrier of meaning between AI and human-level intelligence still stands today.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Should we be concerned that deep learning is not “learning”? We should, if we don’t understand what it implies for real-life applications.&lt;/p&gt;

&lt;p&gt;In the next sections we will explore how neural networks lack the grasp of “rich meanings we humans bring to bear in perception”, illustrating it with some “un-humanlike errors these systems can make; by their difficulties with abstracting and transferring what they have learned; by their lack of commonsense knowledge”.&lt;/p&gt;

&lt;p&gt;You can run the examples used in the text with the Jupyter notebook on &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/machine-learning-but-not-understanding&quot;&gt;this GitHub repository&lt;/a&gt;. The examples use small pictures to run quickly on any computer.&lt;/p&gt;

&lt;h1 id=&quot;telling-squares-and-triangles-apart&quot;&gt;Telling squares and triangles apart&lt;/h1&gt;

&lt;p&gt;We will see how a neural network trained to tell squares and triangles apart behaves.&lt;/p&gt;

&lt;p&gt;For human beings, the pictures below show squares and triangles. Some are small, some are large, some are in a light background, some are in a darker background. But they are all clearly either a square or a triangle in a frame.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-10/squares-triangles.png&quot; alt=&quot;Swuares and triangles&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this section we will go through the typical process of training a neural network to classify squares and triangles:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Get a dataset with labeled pictures of squares and triangles&lt;/li&gt;
  &lt;li&gt;Split the dataset into a training set and a test set&lt;/li&gt;
  &lt;li&gt;Train the network with the training set&lt;/li&gt;
  &lt;li&gt;Validate the neural network accuracy with the test set&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After we are done with that, we will predict similar images to see how the network handles them.&lt;/p&gt;

&lt;h2 id=&quot;the-squares-vs-triangles-training-examples&quot;&gt;The “squares vs. triangles” training examples&lt;/h2&gt;

&lt;p&gt;This is how some of the training images look like. Each picture is a square or a triangle in different positions. The dataset has hundreds of these pictures.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-10/output_12_0.png&quot; alt=&quot;Samples squares and triangles&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-squares-vs-triangles-neural-network&quot;&gt;The “squares vs. triangles” neural network&lt;/h2&gt;

&lt;p&gt;We train a &lt;a href=&quot;https://cs231n.github.io/convolutional-networks/&quot;&gt;convolutional neural network&lt;/a&gt; (CNN) to classify a picture as a “square” or as a “triangle”, using the training examples. We chose a CNN architecture because it is well suited to image classification.&lt;/p&gt;

&lt;p&gt;If you would like to see the details of the training process, see the Jupyter notebook on &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/machine-learning-but-not-understanding&quot;&gt;this GitHub repository&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;how-does-the-neural-network-perform&quot;&gt;How does the neural network perform?&lt;/h2&gt;

&lt;p&gt;Before we started the training process, we set aside 10% of the pictures to use later (67 pictures). They are pictures that the neural network was not trained on. This is the &lt;em&gt;test set&lt;/em&gt;. We use the test set to measure the performance of the neural network.&lt;/p&gt;

&lt;p&gt;A traditional measure of performance is “accuracy”. It measures the percentage of pictures in the test set that were correctly classified.&lt;/p&gt;

&lt;p&gt;First, we ask the neural network to predict what the pictures are (more details on how that happens &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/decision-threshold-effect-on-accuracy&quot;&gt;here&lt;/a&gt;), then we compare with the actual labels and calculate the accuracy.&lt;/p&gt;

&lt;p&gt;Our neural network classified 65 out 67 pictures correctly, for an accuracy of 97%. This is a pretty good accuracy for a relatively small neural network that can be trained quickly.&lt;/p&gt;

&lt;p&gt;Let’s visualize where the neural network made the mistakes. The picture below shows the mistakes with a red border. All other pictures were classified correctly. Below each picture is the neural network’s classification.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-10/output_25_0.png&quot; alt=&quot;Mistakes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Despite the good accuracy, does the neural network understand the concept of what it is learning?&lt;/p&gt;

&lt;h2 id=&quot;when-are-squares-not-squares&quot;&gt;When are squares not squares?&lt;/h2&gt;

&lt;p&gt;When they are larger. At least for this neural network.&lt;/p&gt;

&lt;p&gt;In this section we will use the neural network we just trained to classify a set of squares. But there is a twist to these squares: they are larger than the ones we used in the training set.&lt;/p&gt;

&lt;p&gt;This is how they look like.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-10/output_28_0.png&quot; alt=&quot;Larger squares&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using the neural network, we classify the large squares and calculate the accuracy, just like we did with the test set.&lt;/p&gt;

&lt;p&gt;But this time, out of 77 large squares, only 43 are classified as squares. The other 34 are classified as triangles. With an accuracy of 55.8%, the neural network is barely better than flipping a coin.&lt;/p&gt;

&lt;p&gt;Below are all the squares in this set and how the neural network classified them. The ones with the red border were incorrectly classified as triangles (there are many of them).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-10/output_34_0.png&quot; alt=&quot;Large squares wrongly classified as triangles&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-does-this-experiment-matter&quot;&gt;Why does this experiment matter?&lt;/h2&gt;

&lt;p&gt;The simplest and fastest way to improve this neural network is to increase the size of the training and test sets. In this case, we should add larger squares to the training set and retrain the neural network. It will very likely perform better.&lt;/p&gt;

&lt;p&gt;But this does not address the fundamental problem: &lt;strong&gt;&lt;em&gt;the neural network does not understand the concept of “square”.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Quoting Mitchell again (emphasis added):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The phrase “barrier of meaning” perfectly captures an idea that has permeated this book: humans, in some deep and essential way, understand the situations they encounter, whereas no AI system yet possesses such understanding. While state-of-the-art AI systems have nearly equaled (and in some cases surpassed) humans on certain narrowly defined tasks, &lt;b&gt;these systems all lack a grasp of the rich meanings humans bring to bear in perception, language, and reasoning. This lack of understanding is clearly revealed by the un-humanlike errors these systems can make; by their difficulties with abstracting and transferring what they have learned; by their lack of commonsense knowledge;&lt;/b&gt; … The barrier of meaning between AI and human-level intelligence still stands today.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Even if we collect lots and lots and lots of examples, we are confronted with &lt;strong&gt;&lt;em&gt;the long-tail problem&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“[T]he vast range of possible unexpected situations an AI system could be faced with.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;For example, let’s say we trained our autonomous driving system to recognize a school zone by the warning sign painted on the road (&lt;a href=&quot;https://virtualdriveoftexas.com/texas-school-zones/&quot;&gt;source&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-10/school-spelled-right.png&quot; alt=&quot;School warning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then, one day our autonomous driving system comes across these real-life examples (&lt;a href=&quot;https://www.anyvan.com/blog/whats-going-on/back-to-shcool-for-some/&quot;&gt;source 1&lt;/a&gt;, &lt;a href=&quot;https://www.wibw.com/content/news/School-misspelled-at-Florida-crosswalk-508798331.html?ref=331&quot;&gt;source 2&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-10/school-spelled-wrong-1.png&quot; alt=&quot;School warning mispelled&quot; /&gt;
&lt;img src=&quot;/images/2021-04-10/school-spelled-wrong-2.png&quot; alt=&quot;School warning mispelled&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Any (well, most) human beings would still identify them as warning signs for school zones (presumably, the human would chuckle, then - hopefully - slow down).&lt;/p&gt;

&lt;p&gt;Would the autonomous driving system identify them correctly? The honest answer is “we don’t know”. It depends on how it was trained. Was it given these examples in the training set? In enough quantities to identify the pattern? Did the test set have examples? Were they classified correctly?&lt;/p&gt;

&lt;p&gt;But no matter how comprehensive we make the training and test sets and how methodically we inspect the classification results, we are faced with the fundamental problem: &lt;strong&gt;&lt;em&gt;the neural network does not understand the concept of “school zone warning”&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The autonomous driving system lacks common sense.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“…humans also have a fundamental competence lacking in all current AI systems: common sense. We have vast background knowledge of the world, both its physical and its social aspects.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The neural network may be &lt;em&gt;learning&lt;/em&gt;, but it is definitely not &lt;em&gt;understanding&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;not-understanding-squares---part-2&quot;&gt;Not understanding “squares” - part 2&lt;/h1&gt;

&lt;p&gt;In the first section we changed the shape of an object. In this section we will not change the object. We will change the environment instead.&lt;/p&gt;

&lt;p&gt;We will train a neural network to classify squares and triangles again. This time they are in different environments, represented by different background colors. The squares are in a lighter background and the triangles are on a dark(er) background (we can think of the backgrounds as “twilight” and “night”).&lt;/p&gt;

&lt;p&gt;The picture below shows how they look like.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-10/output_38_0.png&quot; alt=&quot;Darker background&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Following the same steps we used in the first section, we train a neural network to classify the squares and triangles.&lt;/p&gt;

&lt;p&gt;Once the network is trained, we use the test set to calculate the neural network accuracy and find out that it is a perfect 100% accuracy score. All squares and triangles in the test set were classified correctly.&lt;/p&gt;

&lt;p&gt;If you would like to see the details of the training process, see the Jupyter notebook on &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/machine-learning-but-not-understanding&quot;&gt;this GitHub repository&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So far, so good, but…&lt;/p&gt;

&lt;h2 id=&quot;in-the-dark-all-squares-are-triangles&quot;&gt;In the dark, all squares are triangles&lt;/h2&gt;

&lt;p&gt;What happens if the squares are now in the same environment as the triangles (all squares are in the “night” environment)?&lt;/p&gt;

&lt;p&gt;This is how the squares look like in the darker environment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-10/output_47_0.png&quot; alt=&quot;Squares in darker background&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When we ask the neural network to classify these squares, we find out that the performance is now abysmal. The accuracy is 0%. All squares are misclassified as triangles.&lt;/p&gt;

&lt;p&gt;To confirm, we can visualize the predictions. The wrong predictions have a red frame around them (all of them are wrong in this case).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-10/output_52_0.png&quot; alt=&quot;Squares in darker background wrongly predicted as triangles&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;why-does-this-experiment-matter-1&quot;&gt;Why does this experiment matter?&lt;/h2&gt;

&lt;p&gt;The neural network we just trained fails in the same way the first neural network failed: it doesn’t understand the concepts of “square” and “triangle”. It is just looking for any sort of pattern in the training data. It doesn’t know if a pattern makes sense or not, it just knows there is a pattern there.&lt;/p&gt;

&lt;p&gt;In this case, the neural network is very likely learning not from the shape, but from the background (a case of &lt;a href=&quot;https://arxiv.org/abs/1907.02893&quot;&gt;spurious correlation&lt;/a&gt;). It is assuming that a darker background means “triangle” because it doesn’t really understand the concept of what makes a triangle a triangle.&lt;/p&gt;

&lt;p&gt;Sometimes this leads to some funny examples, like the neural network that “learned” to classify land vs. water birds based on the background. The duck on the right was misclassified as a land bird, simply because it was not in its usual water environment (&lt;a href=&quot;https://arxiv.org/abs/2005.04345&quot;&gt;source&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-10/land-and-waterbirds.png&quot; alt=&quot;Water birds vs. land birds&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Other times the mistakes are more consequential, for example, when neural networks misclassify X-rays based on markings left by radiologists in the images. Instead of learning actual attributes of a disease, the neural network “learned” from the marks left behind in the images. Images without such marks may be classified as “healthy”. The consequences can be catastrophic (&lt;a href=&quot;https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf&quot;&gt;source&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-04-10/x-ray-pen-marks.png&quot; alt=&quot;X-ray with pen marks&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;should-we-be-concerned-that-deep-learning-is-not-understanding&quot;&gt;Should we be concerned that deep “learning” is not “understanding”?&lt;/h1&gt;

&lt;p&gt;Mitchell asks the following question in her book:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“but the question remains: Will the fact that these systems lack humanlike understanding inevitably render them fragile, unreliable, and vulnerable to attacks? And how should this factor into our decisions about applying AI systems in the real world?”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Until we achieve humanlike understanding, we should be concerned that neural networks do not generalize well.&lt;/p&gt;

&lt;p&gt;Does it mean we need to stop using neural networks until then? No.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“I think the most worrisome aspect of AI systems in the short term is that we will give them too much autonomy without being fully aware of their limitations and vulnerabilities.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Deep learning has successfully improved our lives. It’s “just” a matter of understanding its limitations, applying it judiciously, for the tasks that it’s well suited.&lt;/p&gt;

&lt;p&gt;To do that we need to educate the general public and, more importantly, the technical community. Too often we hype the next “AI has achieved humanlike performance in [&lt;em&gt;some task here&lt;/em&gt;]”, when in fact we should say “under these specific circumstances, for this specific application, AI has performed well”.&lt;/p&gt;

&lt;h1 id=&quot;source-code-for-the-experiments&quot;&gt;Source code for the experiments&lt;/h1&gt;

&lt;p&gt;The source code for the experiments described here is on &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/machine-learning-but-not-understanding&quot;&gt;this GitHub repository&lt;/a&gt;. It uses small pictures to run quickly on a regular computer.&lt;/p&gt;

&lt;p&gt;Feel free to modify the pictures, the neural network model, and other parameters that affect the results.&lt;/p&gt;

&lt;p&gt;But remember that when the results improve, it’s not the neural network that is learning more all of a sudden. &lt;em&gt;You&lt;/em&gt; are improving it.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Because of the open-ended nature of designing these networks, in general it is not possible to automatically set all the parameters and designs, even with automated search. Often it takes a kind of cabalistic knowledge that students of machine learning gain both from their apprenticeships with experts and from hard-won experience.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;small&quot;&gt;&lt;cite&gt;Melanie Mitchell&lt;/cite&gt; — Artificial Intelligence, a guide for thinking humans&lt;/p&gt;</content><author><name>Christian Garbin</name></author><category term="machine-learning" /><category term="failure" /><summary type="html">In the expression &apos;machine learning&apos;, are the machines actually learning anything? Let&apos;s explore what &apos;learning&apos; means for machine learning, guided by Melanie Mitchell&apos;s book &apos;Artificial Intelligence, a guide for thinking humans. We will see that machines don&apos;t learn in the same way we understand &apos;learn&apos;.</summary></entry><entry><title type="html">What is “model accuracy”, really?</title><link href="https://cgarbin.github.io/decision-threshold-effect-on-accuracy/" rel="alternate" type="text/html" title="What is “model accuracy”, really?" /><published>2021-03-21T00:00:00-04:00</published><updated>2021-03-21T00:00:00-04:00</updated><id>https://cgarbin.github.io/decision-threshold-effect-on-accuracy</id><content type="html" xml:base="https://cgarbin.github.io/decision-threshold-effect-on-accuracy/">&lt;p&gt;In the book &lt;a href=&quot;https://www.h2o.ai/resources/ebook/responsible-machine-learning/&quot;&gt;Responsible Machine Learning&lt;/a&gt;,
when discussing trust and risk, the authors recommend a list of questions to ask to understand the
risk of a machine learning (ML) deployment.&lt;/p&gt;

&lt;p&gt;One of the questions is &lt;strong&gt;“What is the quality of the model? (Accuracy, AUC/ROC, F1)”&lt;/strong&gt;. These
metrics compare correct and incorrect predictions of a model.&lt;/p&gt;

&lt;p&gt;But how exactly a model determines what a correct prediction is?&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Here we will analyze the effect of an important factor a model uses to decide the correct prediction (label) for classification problems, the &lt;strong&gt;decision threshold&lt;/strong&gt;. We will see that without understanding how a model decides what “correct” is, talking about the model’s accuracy
is premature.&lt;/p&gt;

&lt;p&gt;We use &lt;em&gt;accuracy&lt;/em&gt; in this text as the number of correct predictions on the test set, divided by the number of instances in the test set.&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;                 Number of correct predictions
   Accuracy = -----------------------------------
                 Number of instances predicted
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To illustrate the discussion, we will use an image classification model (simplification disclaimer: there are other types of problems, e.g. regression, and other types of models – we are making simplifications to expose the main concept.)&lt;/p&gt;

&lt;p&gt;A typical image classification problem, taught early in machine learning, is digit classification with the
&lt;a href=&quot;http://yann.lecun.com/exdb/mnist/&quot;&gt;MNIST dataset&lt;/a&gt;. The dataset looks like this (a small sample -
the dataset has 70,000 images):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-21/mnist.png&quot; alt=&quot;Sample digits from the MNIST dataset&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In an image classification problem, we train a model to identify an image’s class (label).
In this case, there are ten classes, one for each digit (from zero to nine).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-21/digit-classification-model.png&quot; alt=&quot;Digit classification model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is an actual digit from MNIST. The model correctly classifies it as the digit “2”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-21/digit-classification-example.png&quot; alt=&quot;Classification example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A neural network has several hidden layers to extract (“learn”) features from the images. The very
last layer is the one that classifies the image. In this case, we are classifying ten classes (ten
digits). Therefore the last layer has ten neurons, one for each digit.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-21/classification-layer.png&quot; alt=&quot;Classification layer&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Because we want to know what digit it is, we use &lt;a href=&quot;https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax&quot;&gt;softmax activation&lt;/a&gt;
in the last layer to give us a probability distribution of each class. The model
is confident that the image is a number “2” in the case below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-21/model-classification-certain.png&quot; alt=&quot;Model classification certain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For other images, the model may not be so confident.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-21/model-classification-not-certain.png&quot; alt=&quot;Model classification not certain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In those cases, how should we decide what the label is?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-21/model-classification-how-to-decide.png&quot; alt=&quot;Model classification - how to decide&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most of the time, the class with the largest probability is used as the label. In this example, the
model classifies the image as the digit “2”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-21/model-classification-use-largest.png&quot; alt=&quot;Model classification - largest probability&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But what should the model do when the largest probability is not that high and is close to the
probability of other classes?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-21/model-classification-uncertain.png&quot; alt=&quot;&apos;Model classification - uncertain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the example below, the largest probability is for class “9”, but it is not even 50%, and the
probability for class “4” is not too far behind. The model does not have high confidence in this
prediction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-21/model-classification-uncertain2.png&quot; alt=&quot;Model classification - uncertain&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What should we do in these cases?&lt;/p&gt;

&lt;p&gt;To solve those cases, we usually pick a threshold for the decision. Instead of simply using the class
with the maximum probability, we select the largest probability above the threshold we chose. If we
choose 50% as the threshold, in the number “2” example above we are still able to classify the image
as the number “2”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-21/model-classification-threshold-above.png&quot; alt=&quot;Model classification - above threshold&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But now we no longer classify the ambiguous image as a number “9”. In this case, we would not make
a decision at all.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-21/model-classification-threshold-below.png&quot; alt=&quot;Model classification - below threshold&quot; /&gt;&lt;/p&gt;

&lt;p&gt;But what threshold do we pick?&lt;/p&gt;

&lt;p&gt;It depends. For high-stakes applications where wrong decisions have severe consequences, we want to
be very confident in the model’s prediction.&lt;/p&gt;

&lt;p&gt;For example, for an automatic check deposit application, we want the model to be at least 99%
confident of the prediction. Any image below that threshold is sent for human review.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-21/model-classification-high-stakes.png&quot; alt=&quot;Model classification - high stakes&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;effect-of-different-thresholds&quot;&gt;Effect of different thresholds&lt;/h2&gt;

&lt;p&gt;The higher the threshold for the decision, the fewer images the model can classify. For the model
used in these examples, this is the effect of different thresholds on the model’s accuracy.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Threshold&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Accuracy&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;99.99%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;48.7%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;99.9%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;75.6%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;99%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90.0%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;90%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;96.4%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;80%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;97.8%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;75%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;97.8%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;67%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;98.2%&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;50%&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;98.8%&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;asking-questions-about-accuracy&quot;&gt;Asking questions about “accuracy”&lt;/h2&gt;

&lt;p&gt;The one-line takeaway: &lt;em&gt;to use a model responsibly &lt;strong&gt;we must ask questions&lt;/strong&gt; about how its accuracy
was measured and not just accept published numbers&lt;/em&gt;.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;How predictions are being made: is it probability-based (as in the examples above)? Something
else?&lt;/li&gt;
  &lt;li&gt;What factors control the predictions: is it threshold-based or some other decision (e.g. argmax)?
If it is threshold-based, what are the thresholds?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;“We achieved 99.9% accuracy on [some task here]”&lt;/em&gt; means nothing if it’s not accompanied by a
detailed description of what a “correct prediction” is for the model.&lt;/p&gt;

&lt;h2 id=&quot;what-to-use-instead-of-accuracy&quot;&gt;What to use instead of accuracy?&lt;/h2&gt;

&lt;p&gt;Balanced datasets have similar numbers of instances for each class. For these cases, a better alternative to &lt;em&gt;accuracy&lt;/em&gt; is the &lt;a href=&quot;https://en.wikipedia.org/wiki/Receiver_operating_characteristic&quot;&gt;receiver operating characteristic (ROC) curve&lt;/a&gt; (for a simpler introduction, see &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc&quot;&gt;this page&lt;/a&gt;). The ROC curve shows, at a glance, how a model behaves with different thresholds.&lt;/p&gt;

&lt;p&gt;Imbalanced datasets have a large number of instances for one of the classes and a small number of instances for the other classes.  &lt;em&gt;Accuracy&lt;/em&gt; is an especially flawed metric for imbalanced datasets with a small number of positive class instances. These datasets are typical in healthcare applications where a specific condition affects only a small portion of the population. For example, if a disease affects only 0.1% of the population, a “classifier” that always returns “no disease” without even looking at the data is 99.9% “accurate”. In these cases, &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall&quot;&gt;precision and recall&lt;/a&gt; are better metrics. &lt;em&gt;&lt;a href=&quot;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0118432&quot;&gt;The Precision-Recall Plot is More Informative than the ROC Plot when Evaluating Binary Classifiers on Imbalanced Datasets&lt;/a&gt;&lt;/em&gt; explains in detail, with concrete examples why should prefer the precision-recall plot for imbalanced datasets with a small number of positive instances.&lt;/p&gt;

&lt;p&gt;Every scientific paper that describes a model should publish the ROC or precision-recall curve. Papers that publish only the accuracy for the model, and especially papers that publish the accuracy without specifying the threshold, are, at best, incomplete. At worst, they were written by uninformed machine learning novices.&lt;/p&gt;

&lt;h2 id=&quot;experimenting-with-the-code&quot;&gt;Experimenting with the code&lt;/h2&gt;

&lt;p&gt;The code is available on &lt;a href=&quot;https://github.com/fau-masters-collected-works-cgarbin/decision-threshold-effect-on-accuracy&quot;&gt;this GitHub repository&lt;/a&gt;. You can experiment with different digits and classification thresholds to see the effect on the model’s accuracy.&lt;/p&gt;</content><author><name>Christian Garbin</name></author><category term="machine-learning" /><category term="accuracy" /><category term="roc" /><summary type="html">A model&apos;s accuracy is an incomplete view of of the model&apos;s performance. This article shows how it can be misleading.</summary></entry><entry><title type="html">Fairness in machine learning: a reading list</title><link href="https://cgarbin.github.io/fairness-a-reading-list/" rel="alternate" type="text/html" title="Fairness in machine learning: a reading list" /><published>2021-03-18T00:00:00-04:00</published><updated>2021-03-18T00:00:00-04:00</updated><id>https://cgarbin.github.io/fairness-a-reading-list</id><content type="html" xml:base="https://cgarbin.github.io/fairness-a-reading-list/">&lt;p&gt;This article lists resources to understand concepts and applications of fairness in machine learning (ML).&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;sources-for-people-in-a-hurry&quot;&gt;Sources for people in a hurry&lt;/h2&gt;

&lt;h3 id=&quot;i-can-dedicate-at-most-one-hour-of-my-life-to-fairness-in-ml&quot;&gt;“I can dedicate at most one hour of my life to fairness in ML”&lt;/h3&gt;

&lt;p&gt;In that case, watch the video &lt;a href=&quot;https://fairmlbook.org/tutorial2.html&quot;&gt;21 fairness definitions and their politics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Narayanan is an enganging speaker. He is also well-grounded in the subject, as one of the authors of the &lt;a href=&quot;https://fairmlbook.org/&quot;&gt;fairmlbook.org book (Fairness and Machine Learning: Limitations and Opportunities)&lt;/a&gt;. In one hour he covers the complexity of the topic and the state of the research on it (as of 2018, but the fundamentals of the problem are still the same). This is a good source to understand the complexity of the problem and why solutions are not easy (and sometimes contradictory).&lt;/p&gt;

&lt;!--more--&gt;

&lt;h3 id=&quot;if-i-could-recommend-only-one-source&quot;&gt;“If I could recommend only one source…”&lt;/h3&gt;

&lt;p&gt;Of all the sources below, if I could recommend only one (well, one for concepts, one for visualization, and one for a programming project):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Concepts: &lt;a href=&quot;https://fairmlbook.org/&quot;&gt;Fairness and Machine Learning: Limitations and Opportunities&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Visualization: &lt;a href=&quot;https://pair.withgoogle.com/explorables/measuring-fairness/&quot;&gt;Measuring Fairness explorable&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;In practice (programming): &lt;a href=&quot;https://developers.google.com/machine-learning/crash-course/fairness/video-lecture&quot;&gt;Fairness | Machine Learning Crash Course&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other-sources&quot;&gt;Other sources&lt;/h2&gt;

&lt;h3 id=&quot;conceptual&quot;&gt;Conceptual&lt;/h3&gt;

&lt;p&gt;This section lists sources to understand the concepts of fairness and their implications for society, in no particular order.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.10002&quot;&gt;A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Bias is closely related to fairness. This paper describes a framework to understand sources of bias in machine learning. Once we understand where bias comes from, we are better positioned to eliminate or at least mitigate it. The picture below summarizes the framework, illustrating the sources of bias in machine learning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-03-18/sources-of-bias.png&quot; alt=&quot;A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://fairmlbook.org/&quot;&gt;Fairness and Machine Learning: Limitations and Opportunities&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also known as the “fair ML book”, from the site URL, &lt;a href=&quot;https://fairmlbook.org/&quot;&gt;fairmlbook.org&lt;/a&gt;. A free online book from luminaries of the field. It’s updated frequently. If we could have only one reference for concepts, this would be it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://fairmlclass.github.io/&quot;&gt;CS 294: Fairness in Machine Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;From the authors of &lt;a href=&quot;https://fairmlbook.org/&quot;&gt;fairmlbook.org&lt;/a&gt; (previous item). A note at the top says “For an updated resource, please see fairmlbook.org.” However, it is still a good source because the sequence of lectures is a structured way to understand/explore fairness and has many pointers to other references.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://fairmlbook.org/tutorial2.html&quot;&gt;21 fairness definitions and their politics&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Continuing the &lt;a href=&quot;https://fairmlbook.org/&quot;&gt;fairmlbook.org&lt;/a&gt; theme, this is a one-hour presentation by one of the authors. It is a good source to learn in a short time the complexities of the field (and thus, the complexity of the solutions).&lt;/p&gt;

&lt;h3 id=&quot;keeping-up-with-the-latest-research&quot;&gt;Keeping up with the latest research&lt;/h3&gt;

&lt;p&gt;Visit the &lt;a href=&quot;https://paperswithcode.com/task/fairness&quot;&gt;Papers With Code fairness section&lt;/a&gt; frequently for the latest papers, libraries, and datasets on the topic.&lt;/p&gt;

&lt;h3 id=&quot;recent-survey-papers&quot;&gt;Recent survey papers&lt;/h3&gt;

&lt;p&gt;These are some recent survey papers on fairness as of early 2021. Each has a brief extract to help explain why it is relevant and what aspects it covers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.09635&quot;&gt;A Survey on Bias and Fairness in Machine Learning&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In this survey we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and how they have tried to address them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1903.03425&quot;&gt;The Ethics of AI Ethics – An Evaluation of Guidelines&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Current advances in research, development and application of artificial intelligence (AI) systems have yielded a far-reaching discourse on AI ethics. In consequence, a number of ethics guidelines have been released in recent years. […] Designed as a comprehensive evaluation, this paper analyzes and compares these guidelines highlighting overlaps but also omissions. As a result, I give a detailed overview of the field of AI ethics. Finally, I also examine to what extent the respective ethical principles and values are implemented in the practice of research, development and application of AI systems – and how the effectiveness in the demands of AI ethics can be improved.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1905.06876&quot;&gt;From What to How: An Initial Review of Publicly Available AI Ethics Tools, Methods and Research to Translate Principles into Practices&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[O]ur intention in presenting this research is to contribute to closing the gap between principles and practices by constructing a typology that may help practically-minded developers apply ethics at each stage of the pipeline, and to signal to researchers where further work is needed. The focus is exclusively on Machine Learning, but it is hoped that the results of this research may be easily applicable to other branches of AI. The article outlines the research method for creating this typology, the initial findings, and provides a summary of future research needs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3287560.3287600&quot;&gt;50 Years of Test (Un)fairness: Lessons for Machine Learning&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way towards future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2112.05700&quot;&gt;A Framework for Fairness: A Systematic Review of Existing Fair AI Solutions&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A large portion of fairness research has gone to producing tools that machine learning practitioners can use to audit for bias while designing their algorithms. Nonetheless, there is a lack of application of these fairness solutions in practice. This systematic review provides an in-depth summary of the algorithmic bias issues that have been defined and the fairness solution space that has been proposed. Moreover, this review provides an in-depth breakdown of the caveats to the solution space that have arisen since their release and a taxonomy of needs that have been proposed by machine learning practitioners, fairness researchers, and institutional stakeholders&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;social-and-legal-implications&quot;&gt;Social and legal implications&lt;/h3&gt;

&lt;p&gt;Not all aspects of machine learning fairness are technical. We care about fairness because it affects humans at the individual and societal levels. These are some papers that look at the social and legal implications of fairness in machine learning.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.11869&quot;&gt;This Thing Called Fairness: Disciplinary Confusion Realizing a Value in Technology&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;[T]his paper examines the value of shared vocabularies, analytics, and other tools that facilitate conversations about values in light of these disciplinary specific conceptualizations, the role such tools play in furthering research and practice, outlines different conceptions of “fairness” deployed in discussions about computer systems, and provides an analytic tool for interdisciplinary discussions and collaborations around the concept of fairness. We use a case study of risk assessments in criminal justice applications to both motivate our effort–describing how conflation of different concepts under the banner of “fairness” led to unproductive confusion–and illustrate the value of the fairness analytic by demonstrating how the rigorous analysis it enables can assist in identifying key areas of theoretical, political, and practical misunderstanding or disagreement, and where desired support alignment or collaboration in the absence of consensus.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;My (Christian’s) commentary: highly analytical, with strong definitions of fairness from different points of view (section 5). Then it tackles the controversial COMPAS (Section 7) system and argues, among other points, that analyzing only the algorithm and not the entire system is shortsighted. Appendix A, “An Analytic for applying contested conceptions of fairness in computer systems,” is worth the paper alone.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1912.00761&quot;&gt;On the Legal Compatibility of Fairness Definitions&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Past literature has been effective in demonstrating ideological gaps in machine learning (ML) fairness definitions when considering their use in complex sociotechnical systems. However, we go further to demonstrate that these definitions often misunderstand the legal concepts from which they purport to be inspired, and consequently inappropriately co-opt legal language. In this paper, we demonstrate examples of this misalignment and discuss the differences in ML terminology and their legal counterparts […].&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;healthcare-specific&quot;&gt;Healthcare specific&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6347576/&quot;&gt;Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This Special Communication outlines the potential biases that may be introduced into machine learning–based clinical decision support tools that use electronic health record data and proposes potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;bias-and-fairness&quot;&gt;Bias and fairness&lt;/h2&gt;

&lt;p&gt;Bias is a close cousin to fairness. Read more about bias in &lt;a href=&quot;/bias-data-science-machine-learning/&quot;&gt;this post&lt;/a&gt;.&lt;/p&gt;</content><author><name>Christian Garbin</name></author><category term="machine-learning" /><category term="bias" /><category term="fairness" /><category term="social-impact" /><summary type="html">A list of resources to understand concepts and applications of fairness in machine learning (ML).</summary></entry></feed>